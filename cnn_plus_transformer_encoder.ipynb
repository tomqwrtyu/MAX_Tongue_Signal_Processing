{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package & Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "# import emd\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL_NUMBER = 2\n",
    "WINDOW_SIZE = 200\n",
    "SLIDING_STEP = int(WINDOW_SIZE * 0.25)\n",
    "KEY_CLASS = {0:'undefined action', 1:'up', 2:'down', 3:'left', 4:'right', 5:'quick touch'}\n",
    "ACCEPT_CLASS = [0, 1, 2, 3, 4, 5]\n",
    "CLASS_NUMBER = 5 # 0 is not a class\n",
    "NUM_IMF = 3\n",
    "LABEL_THRESHOLD = 0.8\n",
    "BELIEF_THRESHOLD = 0.8\n",
    "INITIAL_PULSE = 100 # abandon initial pulse data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture related definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size = 1024, d_model = 32):\n",
    "        super().__init__()\n",
    "        def positional_encoding(length, depth):\n",
    "            depth = depth/2\n",
    "\n",
    "            positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "            depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "            angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "            angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "            pos_encoding = np.concatenate(\n",
    "                [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "                axis=-1) \n",
    "\n",
    "            return pos_encoding\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(2048, d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.image.extract_patches(images=x,\n",
    "                                    sizes=[1, CHANNEL_NUMBER, 2, x.shape[-1]],\n",
    "                                    strides=[1, CHANNEL_NUMBER, 1, x.shape[-1]],\n",
    "                                    rates=[1, 1, 1, 1],\n",
    "                                    padding='VALID')\n",
    "        patch_dims = x.shape[-1]\n",
    "        x = tf.reshape(x, [batch_size, x.shape[1] * x.shape[2], patch_dims])\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float16))\n",
    "        pe = self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]\n",
    "        for _ in range(x.shape[1] - 1):\n",
    "            pe = np.concatenate([pe, self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]], axis=1)\n",
    "        x = x + tf.cast(pe, dtype=tf.float16)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        def point_wise_feed_forward_network(d_model, dff):\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(dff, activation='elu'),  # (batch_size, seq_len, dff)\n",
    "                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "            ])\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = d_model)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        attn_output = self.mha(x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class lrs(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicing(x, y):\n",
    "    totalLength = x.shape[0]\n",
    "    assert totalLength == y.shape[0], \"Data numbers not matching with that of labels.\"\n",
    "    if totalLength <= WINDOW_SIZE:\n",
    "        return x, y\n",
    "\n",
    "    y = one_hot(y)\n",
    "    \n",
    "    thresholdWindow = LABEL_THRESHOLD * WINDOW_SIZE\n",
    "    retx = None\n",
    "    rety = None\n",
    "    retUnknown = None\n",
    "    \n",
    "    i = 0\n",
    "    while (totalLength - i) > WINDOW_SIZE:\n",
    "        new = (x[i:(i + WINDOW_SIZE), :])[np.newaxis, :]\n",
    "        \n",
    "        classSum = np.sum(y[i:(i + WINDOW_SIZE)], axis = 0)\n",
    "        maxIdx = np.argmax(classSum)\n",
    "        maxIdx = maxIdx if maxIdx in ACCEPT_CLASS else 0\n",
    "        if classSum[maxIdx] > thresholdWindow:\n",
    "            if not isinstance(retx, np.ndarray):\n",
    "                retx = new.copy()\n",
    "                rety = [maxIdx + 1]\n",
    "            else:\n",
    "                retx = np.concatenate([retx, new], axis=0)\n",
    "                rety.append(maxIdx + 1)\n",
    "        else:\n",
    "            if not isinstance(retUnknown, np.ndarray):\n",
    "                retUnknown = new.copy()\n",
    "            else:\n",
    "                retUnknown = np.concatenate([retUnknown, new], axis=0)\n",
    "\n",
    "        i += SLIDING_STEP\n",
    "        \n",
    "    return np.transpose(retx[:, np.newaxis], (0, 3, 2, 1)), one_hot(rety), np.transpose(retUnknown[:, np.newaxis], (0, 3, 2, 1))\n",
    "\n",
    "def one_hot(arr):\n",
    "    ret = []\n",
    "    for val in arr:\n",
    "        tmp = [0] * CLASS_NUMBER\n",
    "        if val > 0 and val in ACCEPT_CLASS:\n",
    "            tmp[ACCEPT_CLASS.index(val) - 1] = 1\n",
    "        ret.append(np.array(tmp))\n",
    "        \n",
    "    return np.array(ret)\n",
    "\n",
    "def train_test_unknown_split(trainSignal, trainLabel, unknownActions, seed = 343, fold = None,  randomUnknown = True, base = 0.05, rand = 0.1):\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "    for x, y, u in zip(trainSignal, trainLabel, unknownActions):\n",
    "        if (randomUnknown):\n",
    "            if x.shape[0] // CLASS_NUMBER < u.shape[0]:\n",
    "                X_unknown_add = u[np.random.choice(u.shape[0], size = int(x.shape[0] // CLASS_NUMBER), replace = False)]\n",
    "                y_unknown_add = (np.random.rand(int(x.shape[0] // CLASS_NUMBER), CLASS_NUMBER) * rand + base)\n",
    "            else:\n",
    "                X_unknown_add = u\n",
    "                y_unknown_add = (np.random.rand(u.shape[0], CLASS_NUMBER) * rand + base)\n",
    "\n",
    "            Xt = np.concatenate([x, X_unknown_add], axis = 0)\n",
    "            yt = np.concatenate([y, y_unknown_add], axis = 0)\n",
    "            xtr, xte, ytr, yte = train_test_split(Xt, yt, test_size=0.2, random_state=seed)\n",
    "        else:\n",
    "            xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "            \n",
    "        if not isinstance(X_train, np.ndarray):\n",
    "            X_train = xtr\n",
    "            X_test = xte\n",
    "            y_train = ytr\n",
    "            y_test = yte\n",
    "        else:\n",
    "            X_train = np.concatenate([X_train, xtr], axis=0)\n",
    "            X_test = np.concatenate([X_test, xte], axis=0)\n",
    "            y_train = np.concatenate([y_train, ytr], axis=0)\n",
    "            y_test = np.concatenate([y_test, yte], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate(y, r, uw = 3, belief = BELIEF_THRESHOLD, v = True):\n",
    "    gj, bj, bua = {}, {}, {}\n",
    "    bu, tg, tb, lrconfusion, rlconfusion = 0, 0, 0, 0, 0\n",
    "\n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        gj[cls] = 0\n",
    "        bj[cls] = 0\n",
    "        \n",
    "    for r, p in zip(y, r.numpy()):\n",
    "        rm = KEY_CLASS[np.argmax(r) + 1 if any(r) else 0]\n",
    "        pm = KEY_CLASS[np.argmax(p) + 1 if p[np.argmax(p)] > belief else 0]\n",
    "        if rm == pm:\n",
    "            tg += 1\n",
    "            gj[rm] += 1\n",
    "        else:\n",
    "            tb += 1\n",
    "            bj[rm] += 1\n",
    "            if rm == \"undefined action\":\n",
    "                bu += 1\n",
    "                if bua.get(pm, None):\n",
    "                    bua[pm] += 1\n",
    "                else:\n",
    "                    bua[pm] = 1\n",
    "            elif (rm == \"left\") and (pm == \"right\"):\n",
    "                lrconfusion += 1\n",
    "            elif (rm == \"right\") and (pm == \"left\"):\n",
    "                rlconfusion += 1\n",
    "\n",
    "    la, ra, ua = 0, 0, 0\n",
    "    \n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        if v:\n",
    "            print(\"Action: {}, True: {}, False: {}, Accuracy: {:.4f}\".format(cls, gj[cls], bj[cls], gj[cls] / (gj[cls] + bj[cls] + 0.001)))\n",
    "        if cls == \"left\":\n",
    "            la = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "            if v:\n",
    "                print(\"Massive prediction error times: {}, portion: {:.4f}.\".format(lrconfusion, lrconfusion / bj[cls]))\n",
    "        elif cls == \"right\":\n",
    "            ra = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "            if v:\n",
    "                print(\"Massive prediction error times: {}, portion: {:.4f}.\".format(rlconfusion, rlconfusion / bj[cls]))\n",
    "        elif cls == \"undefined action\":\n",
    "            ua = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "    if v:\n",
    "        print(\"Total True: {}, False: {}, Accuracy: {:.4f}\".format(tg, tb, tg / (tg + tb)))\n",
    "        for cls, bp in bua.items():\n",
    "            print(\"Action:{} ,bad prediction times: {}\".format(cls, bp))\n",
    "\n",
    "    return la + ra + uw * ua\n",
    "\n",
    "def ConTradiction_model(inputShape, d_model = 32, convDropRate = 0.5, encDropRate = 0.7):\n",
    "    input = tf.keras.layers.Input(shape = inputShape)\n",
    "    conv = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 3)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(input)\n",
    "    bnorm = tf.keras.layers.BatchNormalization()(conv)\n",
    "    pooling = tf.keras.layers.AveragePooling2D((1, 8), padding='same')(bnorm)\n",
    "    drop = tf.keras.layers.Dropout(convDropRate)(pooling)\n",
    "    conv2 = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 6)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(drop)\n",
    "    bnorm2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    pooling2 = tf.keras.layers.AveragePooling2D((1, 4), padding='same')(bnorm2)\n",
    "    drop2 = tf.keras.layers.Dropout(convDropRate)(pooling2)\n",
    "\n",
    "    #transformer encoder\n",
    "    encoder = EncoderLayer(d_model, 8, 2 * d_model, encDropRate)(drop2)\n",
    "    #Classification\n",
    "    flatten = tf.keras.layers.Flatten()(encoder)\n",
    "    output = tf.keras.layers.Dense(CLASS_NUMBER, activation='softmax')(flatten)\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lrs(d_model, 50)),\n",
    "                    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preprocess and split record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded .\\data\\2023_Apr_11_145938_l5m6r7_record_X.npy and .\\data\\2023_Apr_11_145938_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_11_224807_l5m6r7_record_X.npy and .\\data\\2023_Mar_11_224807_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_12_015657_l5m6r7_record_X.npy and .\\data\\2023_Mar_12_015657_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_14_153445_l5m6r7_record_X.npy and .\\data\\2023_Mar_14_153445_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_14_170710_l5m6r7_record_X.npy and .\\data\\2023_Mar_14_170710_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_21_182116_l5m6r7_record_X.npy and .\\data\\2023_Mar_21_182116_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_29_191038_l5m6r7_record_X.npy and .\\data\\2023_Mar_29_191038_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_29_203224_l5m6r7_record_X.npy and .\\data\\2023_Mar_29_203224_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_165712_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_165712_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_174506_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_174506_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_184318_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_184318_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_184837_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_184837_l5m6r7_record_y.npy.\n",
      "Number of X: 5057, unknown X: 6246\n"
     ]
    }
   ],
   "source": [
    "trainSignalFiles = glob(\".\\\\data\\\\*_record_X.npy\")\n",
    "trainLabelFiles = [x.replace('X', 'y') for x in trainSignalFiles]\n",
    "\n",
    "numX = 0\n",
    "numUX = 0\n",
    "sigPLot = None\n",
    "trainSignal, trainLabel, unknownActions = [], [], []\n",
    "for sfp, lfp in zip(trainSignalFiles, trainLabelFiles):\n",
    "    print(\"loaded {} and {}.\".format(sfp, lfp))\n",
    "    originSignal = np.load(sfp)[INITIAL_PULSE:] #3 - channel\n",
    "    tempSig = np.concatenate([originSignal[:, 0][:, np.newaxis], originSignal[:, 2][:, np.newaxis]], axis=1) #2 - channel\n",
    "    tempLbl = np.load(lfp)[INITIAL_PULSE:]\n",
    "    sigPLot = tempSig if not isinstance(sigPLot, np.ndarray) else np.concatenate([sigPLot, tempSig], axis=0)\n",
    "    X, y, X_unknown = slicing(tempSig, tempLbl)\n",
    "    trainSignal.append(X)\n",
    "    trainLabel.append(y)\n",
    "    unknownActions.append(X_unknown)\n",
    "    numX += X.shape[0]\n",
    "    numUX += X_unknown.shape[0]\n",
    "\n",
    "print(\"Number of X: {}, unknown X: {}\".format(numX, numUX))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_unknown_split(trainSignal, trainLabel, unknownActions, seed = 343,\n",
    "                                                            randomUnknown = True, base = 0.15, rand = 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1), convDropRate=0.4, encDropRate=0.4)\n",
    "model.summary()\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=500,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=200,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "loss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          1.4194211959838867,
          1.2093440294265747,
          1.160888671875,
          1.1479482650756836,
          1.1421008110046387,
          1.1283559799194336,
          1.1109845638275146,
          1.1152451038360596,
          1.1144174337387085,
          1.1062960624694824,
          1.096642255783081,
          1.1024190187454224,
          1.1004867553710938,
          1.099524736404419,
          1.0950685739517212,
          1.0925017595291138,
          1.0920263528823853,
          1.0869773626327515,
          1.0747653245925903,
          1.0833982229232788,
          1.0841457843780518,
          1.0826729536056519,
          1.0920604467391968,
          1.083211898803711,
          1.0748471021652222,
          1.0761908292770386,
          1.0708801746368408,
          1.072638988494873,
          1.0670334100723267,
          1.0653789043426514,
          1.0638402700424194,
          1.0593154430389404,
          1.0604082345962524,
          1.0521775484085083,
          1.054725170135498,
          1.0594792366027832,
          1.0540109872817993,
          1.0549633502960205,
          1.052635908126831,
          1.048984169960022,
          1.0522249937057495,
          1.0457122325897217,
          1.0466279983520508,
          1.051023244857788,
          1.0513657331466675,
          1.030326247215271,
          1.0418727397918701,
          1.0377347469329834,
          1.0361196994781494,
          1.0343321561813354,
          1.0246145725250244,
          1.0299068689346313,
          1.021356463432312,
          1.0298317670822144,
          1.0137101411819458,
          1.029579758644104,
          1.017392635345459,
          1.0062862634658813,
          1.0164369344711304,
          1.0111550092697144,
          1.013420581817627,
          1.0064467191696167,
          1.008277416229248,
          1.0100830793380737,
          1.0021064281463623,
          0.9987329840660095,
          1.0032429695129395,
          0.9977903962135315,
          0.9928296208381653,
          0.9984252452850342,
          0.9896132349967957,
          0.9848232865333557,
          0.9775024056434631,
          0.9813219904899597,
          0.9804224967956543,
          0.9805380702018738,
          0.9797839522361755,
          0.9844402074813843,
          0.9714028835296631,
          0.9718732833862305,
          0.9715072512626648,
          0.9760648608207703,
          0.9770154356956482,
          0.9691792726516724,
          0.9695522785186768,
          0.9699513912200928,
          0.9684662222862244,
          0.9550849795341492,
          0.962126612663269,
          0.9595168828964233,
          0.9593043327331543,
          0.9582155346870422,
          0.964652955532074,
          0.9605585932731628,
          0.9514210820198059,
          0.9582659006118774,
          0.9485893249511719,
          0.9517638087272644,
          0.9418961405754089,
          0.9617244601249695,
          0.9444725513458252,
          0.9524930715560913,
          0.9421778321266174,
          0.9391845464706421,
          0.9418432712554932,
          0.9431319832801819,
          0.9407278895378113,
          0.935065746307373,
          0.9450725317001343,
          0.9400513172149658,
          0.9383741021156311,
          0.9441179633140564,
          0.933660626411438,
          0.940699577331543,
          0.9364383220672607,
          0.9401180744171143,
          0.9325833320617676,
          0.9292615652084351,
          0.9286458492279053,
          0.9251344799995422,
          0.9308852553367615,
          0.9250659942626953,
          0.9267065525054932,
          0.9291898608207703,
          0.9239644408226013,
          0.9278987646102905,
          0.9113307595252991,
          0.9130171537399292,
          0.9280918836593628,
          0.9125877618789673,
          0.9175401926040649,
          0.9160283207893372,
          0.9148486852645874,
          0.9152498245239258,
          0.9123937487602234,
          0.9183872938156128,
          0.9114672541618347,
          0.9152429699897766,
          0.9073289036750793,
          0.9068044424057007,
          0.9043565392494202,
          0.913503885269165,
          0.90578693151474,
          0.9140575528144836,
          0.9098421335220337,
          0.9087007641792297,
          0.9059456586837769,
          0.9005092978477478,
          0.9125443696975708,
          0.9012436866760254,
          0.901327908039093,
          0.8983871340751648,
          0.9027873277664185,
          0.9084346294403076,
          0.9036370515823364,
          0.9001131653785706,
          0.9090115427970886,
          0.8890283107757568,
          0.8982270359992981,
          0.906485915184021,
          0.9038122296333313,
          0.8984848260879517,
          0.9000874161720276,
          0.8875928521156311,
          0.8907192945480347,
          0.8980254530906677,
          0.899196445941925,
          0.8878387808799744,
          0.8855082988739014,
          0.8915578126907349,
          0.883586585521698,
          0.8948698043823242,
          0.8962238430976868,
          0.8867022395133972,
          0.892304003238678,
          0.8859666585922241,
          0.8842144012451172,
          0.8778140544891357,
          0.8865165710449219,
          0.891701877117157,
          0.8865984678268433,
          0.8903829455375671,
          0.8855158090591431,
          0.8778085112571716,
          0.8824862837791443,
          0.8811793327331543,
          0.8808078765869141,
          0.8778702616691589,
          0.8791421055793762,
          0.8803712129592896,
          0.8816892504692078,
          0.88958740234375,
          0.8814992904663086,
          0.8688422441482544,
          0.8746525645256042,
          0.8689606785774231,
          0.8745743036270142,
          0.8745874762535095,
          0.8781255483627319,
          0.8692061901092529,
          0.8688875436782837,
          0.8666017055511475,
          0.8636787533760071,
          0.8673579692840576,
          0.8697471022605896,
          0.8698751330375671,
          0.8611516356468201,
          0.8773120641708374,
          0.867321252822876,
          0.877461850643158,
          0.8764026761054993,
          0.8721110224723816,
          0.8716839551925659,
          0.8729549050331116,
          0.8666876554489136,
          0.8668361306190491,
          0.8660517930984497,
          0.8579022884368896,
          0.8774840235710144,
          0.8647903203964233,
          0.8665482997894287,
          0.8641985654830933,
          0.8619565367698669,
          0.8601873517036438,
          0.8615378737449646,
          0.8663153052330017,
          0.8565462231636047,
          0.8611696362495422,
          0.8542923331260681,
          0.8711763620376587,
          0.8538752794265747,
          0.8547324538230896,
          0.8540918231010437,
          0.8613187670707703,
          0.8550190329551697,
          0.8581622242927551,
          0.8555567860603333,
          0.8592004179954529,
          0.867405891418457,
          0.8494380116462708,
          0.8527114391326904,
          0.8451105952262878,
          0.8565571308135986,
          0.8589432239532471,
          0.8538881540298462,
          0.8621094822883606,
          0.8519864082336426,
          0.8445814251899719,
          0.8500639200210571,
          0.8607956171035767,
          0.8539460897445679,
          0.8504539728164673,
          0.8506621718406677,
          0.8479228019714355,
          0.8354949355125427,
          0.8605014681816101,
          0.8529470562934875,
          0.8452906012535095,
          0.8469754457473755,
          0.8545742630958557,
          0.8523675799369812,
          0.8417150974273682,
          0.8527752161026001,
          0.8465693593025208,
          0.8480713963508606,
          0.8352116942405701,
          0.8444918394088745,
          0.8288277387619019,
          0.8436592221260071,
          0.8481919765472412,
          0.841632068157196,
          0.8393430709838867,
          0.84675532579422,
          0.8491612672805786,
          0.8446727395057678,
          0.8378430008888245,
          0.8336880207061768,
          0.8382916450500488,
          0.836929202079773,
          0.8346706032752991,
          0.8401070833206177,
          0.8359887599945068,
          0.8383837342262268,
          0.8366532325744629,
          0.8217642307281494,
          0.8304087519645691,
          0.8404165506362915,
          0.8429042100906372,
          0.841117262840271,
          0.8422092795372009,
          0.8415764570236206,
          0.8381922245025635,
          0.8470870852470398,
          0.8442661762237549,
          0.8350844383239746,
          0.8294180631637573,
          0.8276694416999817,
          0.8353816270828247,
          0.8315027952194214,
          0.8389099836349487,
          0.8373590111732483,
          0.8292874097824097,
          0.8474259376525879,
          0.8322511315345764,
          0.8431637287139893,
          0.8423571586608887,
          0.8301571011543274,
          0.8396259546279907,
          0.8223122954368591,
          0.8393877148628235,
          0.8200065493583679,
          0.828989565372467,
          0.8295324444770813,
          0.820691704750061,
          0.8358588218688965,
          0.8330961465835571,
          0.8309844732284546,
          0.8178107738494873,
          0.8344840407371521,
          0.8322768807411194,
          0.8238734006881714,
          0.8427030444145203,
          0.8340597152709961,
          0.823016345500946,
          0.8252055644989014,
          0.8256493210792542,
          0.8297922015190125,
          0.8260000348091125,
          0.8191161155700684,
          0.824344277381897,
          0.8239717483520508,
          0.8257421255111694,
          0.8260595798492432,
          0.8283556699752808,
          0.8154565691947937,
          0.8261664509773254,
          0.8251198530197144,
          0.8251919150352478,
          0.8249838352203369,
          0.8224276304244995,
          0.8085159659385681,
          0.8170683979988098,
          0.8225225210189819,
          0.8190708756446838,
          0.8202897310256958,
          0.8285965323448181,
          0.8116638660430908,
          0.8069943785667419,
          0.8137324452400208,
          0.8289821147918701,
          0.8086210489273071,
          0.8244601488113403,
          0.8049172759056091,
          0.8106892108917236,
          0.8185189366340637,
          0.8268522620201111,
          0.8134741187095642,
          0.8077154159545898,
          0.8206708431243896,
          0.8155517578125,
          0.8157315850257874,
          0.8075005412101746,
          0.807045578956604,
          0.8048191666603088,
          0.8172780275344849,
          0.8114511966705322,
          0.8175423741340637,
          0.805473268032074,
          0.8130517601966858,
          0.8105830550193787,
          0.8082227110862732,
          0.8081891536712646,
          0.8007412552833557,
          0.8093876838684082,
          0.8032968044281006,
          0.8108747005462646,
          0.8003437519073486,
          0.8130805492401123,
          0.8221614360809326,
          0.7978681921958923,
          0.8178233504295349,
          0.8066625595092773,
          0.8149188160896301,
          0.8066225051879883,
          0.8015812039375305,
          0.8022266626358032,
          0.8001936674118042,
          0.8034690618515015,
          0.8012147545814514,
          0.8032769560813904,
          0.8013163805007935,
          0.7989912033081055,
          0.794589638710022,
          0.7928694486618042,
          0.8022096157073975,
          0.8027769923210144,
          0.798469066619873,
          0.8043317198753357,
          0.8017817735671997,
          0.802299976348877,
          0.8028330206871033,
          0.8078755736351013,
          0.8150835633277893,
          0.8036580681800842,
          0.7946559190750122,
          0.8073382377624512,
          0.7964522242546082,
          0.8027980327606201,
          0.8057090640068054,
          0.8029183149337769,
          0.8035579919815063,
          0.7993876338005066,
          0.809298038482666,
          0.7976963520050049,
          0.7956182956695557,
          0.8052100539207458,
          0.7965065240859985,
          0.792942464351654,
          0.7940305471420288,
          0.7934899926185608,
          0.7782620787620544,
          0.7979291081428528,
          0.7958226799964905,
          0.8054855465888977,
          0.7981602549552917,
          0.7971063852310181,
          0.8032524585723877,
          0.8055406808853149,
          0.8017017245292664,
          0.7889156937599182,
          0.7990171313285828,
          0.7936965823173523,
          0.7957568764686584,
          0.7971944212913513,
          0.7930307984352112,
          0.8035544157028198,
          0.795130729675293,
          0.7929443717002869,
          0.7820230722427368,
          0.8015986680984497,
          0.7850048542022705,
          0.7902317643165588,
          0.7829352617263794,
          0.7910762429237366,
          0.7903831005096436,
          0.7985464334487915,
          0.7844939827919006,
          0.7897680997848511,
          0.797816276550293,
          0.7892791032791138,
          0.7876056432723999,
          0.7926697731018066,
          0.7919104099273682,
          0.7873064279556274,
          0.7902643084526062,
          0.786408543586731,
          0.7874069809913635,
          0.790782630443573,
          0.7786521315574646,
          0.7922248840332031,
          0.7833343148231506,
          0.7775716185569763,
          0.7807498574256897,
          0.7922220826148987,
          0.7950985431671143,
          0.7954927682876587,
          0.7804633378982544,
          0.7914502024650574,
          0.785647451877594,
          0.7888975739479065,
          0.7919026017189026,
          0.7822325229644775,
          0.7944635152816772,
          0.7895881533622742,
          0.7787287831306458,
          0.7739438414573669,
          0.7819458842277527,
          0.7759765386581421,
          0.780970573425293,
          0.782423198223114,
          0.7809162735939026,
          0.7706181406974792,
          0.7777954339981079,
          0.79085373878479,
          0.774121105670929,
          0.7844577431678772,
          0.7761848568916321,
          0.7811226844787598,
          0.7802589535713196,
          0.7828898429870605,
          0.77951979637146,
          0.7647368311882019,
          0.7851998805999756,
          0.7691885232925415,
          0.7753943800926208,
          0.7795641422271729,
          0.7843027114868164,
          0.77528315782547,
          0.7870199084281921,
          0.7764600515365601
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=val_loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "val_loss",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "val_loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          1.2462217807769775,
          1.1359949111938477,
          1.1274049282073975,
          1.1415228843688965,
          1.114088535308838,
          1.1103836297988892,
          1.115544080734253,
          1.1845999956130981,
          1.148361325263977,
          1.1169764995574951,
          1.1363047361373901,
          1.1022549867630005,
          1.086881160736084,
          1.1096702814102173,
          1.1145001649856567,
          1.093717098236084,
          1.085803508758545,
          1.0985493659973145,
          1.0676146745681763,
          1.1252928972244263,
          1.0852843523025513,
          1.0781097412109375,
          1.0793030261993408,
          1.080174446105957,
          1.0735929012298584,
          1.0895456075668335,
          1.0660852193832397,
          1.102185606956482,
          1.0665748119354248,
          1.0639668703079224,
          1.0735535621643066,
          1.0373882055282593,
          1.0565234422683716,
          1.0403740406036377,
          1.0910602807998657,
          1.0527737140655518,
          1.0505292415618896,
          1.0563606023788452,
          1.0516455173492432,
          1.051952838897705,
          1.022814154624939,
          1.042184591293335,
          1.0461608171463013,
          1.05135178565979,
          1.0606220960617065,
          1.032585620880127,
          1.0253336429595947,
          1.005578875541687,
          1.0288350582122803,
          0.9913607835769653,
          1.0052908658981323,
          1.0518107414245605,
          1.0225275754928589,
          0.9996457099914551,
          0.9884110689163208,
          0.9893547296524048,
          1.0084105730056763,
          0.9975324869155884,
          0.9919822812080383,
          1.0080909729003906,
          0.9870234131813049,
          0.9866280555725098,
          0.9709773659706116,
          1.002153992652893,
          0.9816489219665527,
          0.9918261766433716,
          0.9871401786804199,
          0.9803842902183533,
          0.9984338283538818,
          0.9928162097930908,
          0.973291277885437,
          0.9970473647117615,
          1.0159859657287598,
          0.9559755325317383,
          1.004351258277893,
          0.9744222164154053,
          0.9922942519187927,
          0.9624732732772827,
          0.9812987446784973,
          0.9833913445472717,
          0.972963809967041,
          0.9747602343559265,
          0.9542283415794373,
          0.9533122777938843,
          0.9371039271354675,
          0.9474102854728699,
          0.9601060748100281,
          0.9393520355224609,
          0.9246849417686462,
          0.9387076497077942,
          0.9343370199203491,
          0.9398103952407837,
          0.9331956505775452,
          0.9445357322692871,
          0.9341514706611633,
          0.9462517499923706,
          0.9448572993278503,
          0.923053503036499,
          0.962166428565979,
          0.9296171069145203,
          0.9215496182441711,
          0.9221628904342651,
          0.9397416114807129,
          0.9188690781593323,
          0.9356150031089783,
          0.8930818438529968,
          0.9069942831993103,
          0.9177836775779724,
          0.9489060044288635,
          0.9181346297264099,
          0.9228655099868774,
          0.9213963747024536,
          0.9399290680885315,
          0.913352370262146,
          0.9115234017372131,
          0.9213228225708008,
          0.9294523596763611,
          0.9137135148048401,
          0.9105905890464783,
          0.9022520780563354,
          0.8956037759780884,
          0.8950619697570801,
          0.9085389375686646,
          0.922258734703064,
          0.8967341184616089,
          0.9145387411117554,
          0.9038376808166504,
          0.902023196220398,
          0.8872379064559937,
          0.8963913917541504,
          0.8921864628791809,
          0.8870484232902527,
          0.9002261161804199,
          0.9023910760879517,
          0.9139387011528015,
          0.901310920715332,
          0.8901875019073486,
          0.9084656238555908,
          0.910837709903717,
          0.893883466720581,
          0.8719841241836548,
          0.8664758205413818,
          0.9058660268783569,
          0.8818327188491821,
          0.8862906694412231,
          0.8867102265357971,
          0.9089000225067139,
          0.9020956754684448,
          0.8980600833892822,
          0.8796164989471436,
          0.9021664261817932,
          0.8999866247177124,
          0.8806114196777344,
          0.8802387118339539,
          0.8879526853561401,
          0.9104375243186951,
          0.8882628083229065,
          0.8957562446594238,
          0.8778656721115112,
          0.8697869777679443,
          0.8647892475128174,
          0.8898215889930725,
          0.8806542754173279,
          0.8508076071739197,
          0.8724141716957092,
          0.862736701965332,
          0.8727564215660095,
          0.8654651045799255,
          0.882853627204895,
          0.8616197109222412,
          0.8710272312164307,
          0.8642503619194031,
          0.8721684217453003,
          0.8751113414764404,
          0.8629458546638489,
          0.8827029466629028,
          0.88155198097229,
          0.8927863836288452,
          0.873590350151062,
          0.867941677570343,
          0.8863603472709656,
          0.847447395324707,
          0.8592296838760376,
          0.8800011873245239,
          0.8678975701332092,
          0.8882772922515869,
          0.8740934729576111,
          0.87073814868927,
          0.874747633934021,
          0.876775860786438,
          0.8911789655685425,
          0.8722101449966431,
          0.886005699634552,
          0.8712707161903381,
          0.8746828436851501,
          0.8403486013412476,
          0.8937891125679016,
          0.8462609052658081,
          0.8592513203620911,
          0.8994942903518677,
          0.8714931607246399,
          0.8573358058929443,
          0.8852129578590393,
          0.8671391606330872,
          0.8725190758705139,
          0.88851398229599,
          0.8769861459732056,
          0.852655827999115,
          0.8586276173591614,
          0.8469613194465637,
          0.851157546043396,
          0.8750941753387451,
          0.8898604512214661,
          0.8960429430007935,
          0.8638153672218323,
          0.8946879506111145,
          0.8606938123703003,
          0.8675989508628845,
          0.8585595488548279,
          0.8946809768676758,
          0.8410692811012268,
          0.8554561138153076,
          0.8444174528121948,
          0.8877296447753906,
          0.8406456112861633,
          0.8375558257102966,
          0.8941007256507874,
          0.9022061228752136,
          0.8638607859611511,
          0.8681650757789612,
          0.8544989824295044,
          0.8448184132575989,
          0.8446573615074158,
          0.8307316899299622,
          0.8511013984680176,
          0.8577037453651428,
          0.8533204197883606,
          0.8695927858352661,
          0.8604370951652527,
          0.8737503290176392,
          0.8685489296913147,
          0.8626457452774048,
          0.8492358326911926,
          0.8520824909210205,
          0.8542083501815796,
          0.8383649587631226,
          0.8477838635444641,
          0.864687442779541,
          0.8553873300552368,
          0.8417345881462097,
          0.8762606382369995,
          0.854900598526001,
          0.8274720907211304,
          0.8453414440155029,
          0.838495135307312,
          0.8395586013793945,
          0.8786388635635376,
          0.8464906215667725,
          0.8517104387283325,
          0.836016058921814,
          0.8395484685897827,
          0.8200673460960388,
          0.8333646059036255,
          0.8708020448684692,
          0.8615297079086304,
          0.8584985136985779,
          0.840808093547821,
          0.8722215890884399,
          0.8309375047683716,
          0.8570188283920288,
          0.857765793800354,
          0.8541428446769714,
          0.8468425869941711,
          0.8516560792922974,
          0.8483358025550842,
          0.8748094439506531,
          0.867941677570343,
          0.8455838561058044,
          0.859649658203125,
          0.8392940759658813,
          0.848228394985199,
          0.8601199388504028,
          0.8673425912857056,
          0.8585103750228882,
          0.8847453594207764,
          0.8333878517150879,
          0.8523368239402771,
          0.8399955630302429,
          0.8522850871086121,
          0.8885948657989502,
          0.8826364874839783,
          0.8522680401802063,
          0.8704234957695007,
          0.8637917041778564,
          0.8442203402519226,
          0.8471976518630981,
          0.8616989254951477,
          0.8691823482513428,
          0.8776149153709412,
          0.8683767914772034,
          0.8657886385917664,
          0.8617762923240662,
          0.8466511964797974,
          0.846394419670105,
          0.8344683647155762,
          0.8312740921974182,
          0.8372150659561157,
          0.8506556749343872,
          0.8297405242919922,
          0.8565231561660767,
          0.8298053741455078,
          0.8707288503646851,
          0.8487175703048706,
          0.8569941520690918,
          0.8491516709327698,
          0.8430085778236389,
          0.8721036911010742,
          0.8429409265518188,
          0.8387129902839661,
          0.8521144986152649,
          0.8737211227416992,
          0.8580792546272278,
          0.8474934697151184,
          0.823635458946228,
          0.8317300081253052,
          0.8587406873703003,
          0.8482438325881958,
          0.8444200158119202,
          0.8505731821060181,
          0.8986037969589233,
          0.8557426929473877,
          0.8355337381362915,
          0.8566934466362,
          0.8564673662185669,
          0.8384870290756226,
          0.8561134934425354,
          0.8536882400512695,
          0.8754212856292725,
          0.8393270969390869,
          0.8425787687301636,
          0.8432605266571045,
          0.8637210726737976,
          0.8566024899482727,
          0.8357958793640137,
          0.8530502319335938,
          0.8304213285446167,
          0.846504271030426,
          0.8763154745101929,
          0.8470773100852966,
          0.8781645894050598,
          0.847504198551178,
          0.8197938799858093,
          0.8516064286231995,
          0.8374935388565063,
          0.8327428698539734,
          0.8262690305709839,
          0.8235734701156616,
          0.8213003873825073,
          0.8285220265388489,
          0.8247881531715393,
          0.8492396473884583,
          0.8228186964988708,
          0.8388909101486206,
          0.8711630702018738,
          0.8448446393013,
          0.8516138792037964,
          0.833222508430481,
          0.8243989944458008,
          0.8537124395370483,
          0.8424793481826782,
          0.8464091420173645,
          0.8730376362800598,
          0.8569157123565674,
          0.8193378448486328,
          0.8604304194450378,
          0.8302182555198669,
          0.8483607769012451,
          0.8452597856521606,
          0.8286221027374268,
          0.8439435958862305,
          0.8576762676239014,
          0.8585835695266724,
          0.8201075792312622,
          0.829754114151001,
          0.8435460925102234,
          0.8243524432182312,
          0.8501941561698914,
          0.846797525882721,
          0.8464666604995728,
          0.8631220459938049,
          0.8674976229667664,
          0.8233606219291687,
          0.8723188638687134,
          0.8647457957267761,
          0.8483871817588806,
          0.8422717452049255,
          0.8702754974365234,
          0.8375421762466431,
          0.8236774206161499,
          0.8588684797286987,
          0.8525760769844055,
          0.8591834306716919,
          0.8473660945892334,
          0.8696240782737732,
          0.8501320481300354,
          0.8210200667381287,
          0.8366466760635376,
          0.8550138473510742,
          0.8517816662788391,
          0.854824423789978,
          0.8306804299354553,
          0.8773726224899292,
          0.8348550200462341,
          0.8446515202522278,
          0.8463701605796814,
          0.8516014218330383,
          0.8682003617286682,
          0.8418817520141602,
          0.8519125580787659,
          0.8260504603385925,
          0.8586196899414062,
          0.8517887592315674,
          0.8472487926483154,
          0.8358571529388428,
          0.8391410112380981,
          0.8245181441307068,
          0.8765058517456055,
          0.8362221121788025,
          0.835955798625946,
          0.8440226316452026,
          0.8315805792808533,
          0.8683786988258362,
          0.864545464515686,
          0.8429474830627441,
          0.8662993907928467,
          0.8574065566062927,
          0.8330862522125244,
          0.8535921573638916,
          0.8303505778312683,
          0.8460185527801514,
          0.8501383662223816,
          0.8337398171424866,
          0.8238222599029541,
          0.847022533416748,
          0.8504292368888855,
          0.8419186472892761,
          0.8405791521072388,
          0.8429103493690491,
          0.830438494682312,
          0.839313268661499,
          0.8319579362869263,
          0.8790473937988281,
          0.8483416438102722,
          0.7934356331825256,
          0.8444057106971741,
          0.8262090086936951,
          0.875817060470581,
          0.8509991765022278,
          0.8738364577293396,
          0.843206524848938,
          0.8283819556236267,
          0.8465886116027832,
          0.8316551446914673,
          0.8484598994255066,
          0.8261575102806091,
          0.8133847117424011,
          0.8631576895713806,
          0.8647423982620239,
          0.8350550532341003,
          0.853704571723938,
          0.8215804696083069,
          0.8134405612945557,
          0.8139874339103699,
          0.8492947220802307,
          0.8304980397224426,
          0.823075532913208,
          0.8521100878715515,
          0.8516988158226013,
          0.8393051028251648,
          0.8642099499702454,
          0.8638520836830139,
          0.8397509455680847,
          0.8813391327857971,
          0.826971173286438,
          0.8428956866264343,
          0.8183706998825073,
          0.8282971978187561,
          0.8214345574378967,
          0.8453587889671326,
          0.852647602558136,
          0.8274092674255371,
          0.8540465235710144,
          0.8611814975738525,
          0.8468520045280457,
          0.8611593246459961,
          0.8320444226264954,
          0.8740170001983643,
          0.8298052549362183,
          0.8885483741760254,
          0.85651695728302
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = np.array(history.history['loss']).flatten()\n",
    "valf1 = np.array(history.history['val_loss']).flatten()\n",
    "px.line(pd.DataFrame(np.array([f1, valf1]).T, columns=['loss', 'val_loss'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model/ATJaw0414_500_C2_W200_T9769/\", save_format=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unknown = None\n",
    "for u in unknownActions:\n",
    "    if not isinstance(X_unknown, np.ndarray):\n",
    "        X_unknown = u\n",
    "    else:\n",
    "        X_unknown = np.concatenate([X_unknown, u], axis=0)\n",
    "All_X = np.concatenate([X_train, X_test, X_unknown])\n",
    "All_y = np.concatenate([y_train, y_test, np.array([[0] * CLASS_NUMBER for _ in range(X_unknown.shape[0])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(All_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: undefined action, True: 5964, False: 282, Accuracy: 0.9549\n",
      "Action: up, True: 41, False: 529, Accuracy: 0.0719\n",
      "Action: down, True: 53, False: 408, Accuracy: 0.1150\n",
      "Action: left, True: 790, False: 1728, Accuracy: 0.3137\n",
      "Massive prediction error times: 6, portion: 0.0035.\n",
      "Action: right, True: 785, False: 1500, Accuracy: 0.3435\n",
      "Massive prediction error times: 2, portion: 0.0013.\n",
      "Action: quick touch, True: 0, False: 230, Accuracy: 0.0000\n",
      "Total True: 7633, False: 4677, Accuracy: 0.6201\n",
      "Action:right ,bad prediction times: 132\n",
      "Action:left ,bad prediction times: 139\n",
      "Action:down ,bad prediction times: 4\n",
      "Action:up ,bad prediction times: 7\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(All_y, res, belief = 0.9769)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 : [0.89991, 3.593052191864552] 0.7999200000000001 0.89991\n",
      "iteration 1 : [0.889911, 3.593184301729906] 0.879912 0.889911\n",
      "iteration 2 : [0.8869113000000001, 3.595544101071381] 0.8859114 0.8869113000000001\n"
     ]
    }
   ],
   "source": [
    "#Grid search\n",
    "parameterSelection = []\n",
    "lb, ub = 0, 0.9999\n",
    "best = None\n",
    "for n in range(3):\n",
    "    for i in range(10):\n",
    "        threshold = lb + (i + 1) * 0.1 * (ub - lb)\n",
    "        parameterSelection.append([threshold, evaluate(All_y, res, uw = 3, belief = threshold, v = False)])\n",
    "    parameterSelection.sort(key=lambda x:x[1])\n",
    "    lb, ub = parameterSelection[-2][0], parameterSelection[-1][0]\n",
    "    best = parameterSelection[-1]\n",
    "    print(\"iteration\", n, \":\", best, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1))\n",
    "test.load_weights('./model/' + \"ATJaw0414_500_C2_W200_T9769\" + '/')\n",
    "res = test(All_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_21_184531_l5m6r7_record_X.npy')[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_21_184531_l5m6r7_record_y.npy')[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, u = slicing(np.load('./data/2023_Apr_11_145938_l5m6r7_record_X.npy'), np.load('./data/2023_Apr_11_145938_l5m6r7_record_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model(np.concatenate([X, u]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: undefined action, True: 243, False: 4, Accuracy: 0.9838\n",
      "Action: up, True: 0, False: 0, Accuracy: 0.0000\n",
      "Action: down, True: 0, False: 0, Accuracy: 0.0000\n",
      "Action: left, True: 36, False: 75, Accuracy: 0.3243\n",
      "Massive prediction error times: 0, portion: 0.0000.\n",
      "Action: right, True: 28, False: 56, Accuracy: 0.3333\n",
      "Massive prediction error times: 0, portion: 0.0000.\n",
      "Action: quick touch, True: 0, False: 0, Accuracy: 0.0000\n",
      "Total True: 307, False: 135, Accuracy: 0.6946\n",
      "Action:left ,bad prediction times: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.609055822676756"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(np.concatenate([y, np.zeros((u.shape[0], CLASS_NUMBER))]), r, belief = 0.9809)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package & Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "# import emd\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL_NUMBER = 3\n",
    "WINDOW_SIZE = 250\n",
    "SLIDING_STEP = int(WINDOW_SIZE * 0.25)\n",
    "KEY_CLASS = {0:'undefined action', 1:'up', 2:'down', 3:'left', 4:'right', 5:'quick touch'}\n",
    "CLASS_NUMBER = 5 # 0 is not a class\n",
    "NUM_IMF = 3\n",
    "LABEL_THRESHOLD = 0.8\n",
    "BELIEF_THRESHOLD = 0.8\n",
    "INITIAL_PULSE = 100 # abandon initial pulse data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture related definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size = 1024, d_model = 32):\n",
    "        super().__init__()\n",
    "        def positional_encoding(length, depth):\n",
    "            depth = depth/2\n",
    "\n",
    "            positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "            depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "            angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "            angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "            pos_encoding = np.concatenate(\n",
    "                [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "                axis=-1) \n",
    "\n",
    "            return pos_encoding\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(2048, d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.image.extract_patches(images=x,\n",
    "                                    sizes=[1, CHANNEL_NUMBER, 2, x.shape[-1]],\n",
    "                                    strides=[1, CHANNEL_NUMBER, 1, x.shape[-1]],\n",
    "                                    rates=[1, 1, 1, 1],\n",
    "                                    padding='VALID')\n",
    "        patch_dims = x.shape[-1]\n",
    "        x = tf.reshape(x, [batch_size, x.shape[1] * x.shape[2], patch_dims])\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float16))\n",
    "        pe = self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]\n",
    "        for _ in range(x.shape[1] - 1):\n",
    "            pe = np.concatenate([pe, self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]], axis=1)\n",
    "        x = x + tf.cast(pe, dtype=tf.float16)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        def point_wise_feed_forward_network(d_model, dff):\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(dff, activation='elu'),  # (batch_size, seq_len, dff)\n",
    "                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "            ])\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = d_model)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        attn_output = self.mha(x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class lrs(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicing(x, y):\n",
    "    totalLength = x.shape[0]\n",
    "    assert totalLength == y.shape[0], \"Data numbers not matching with that of labels.\"\n",
    "    if totalLength <= WINDOW_SIZE:\n",
    "        return x, y\n",
    "\n",
    "    y = one_hot(y)\n",
    "    \n",
    "    thresholdWindow = LABEL_THRESHOLD * WINDOW_SIZE\n",
    "    retx = None\n",
    "    rety = None\n",
    "    retUnknown = None\n",
    "    \n",
    "    i = 0\n",
    "    while (totalLength - i) > WINDOW_SIZE:\n",
    "        new = (x[i:(i + WINDOW_SIZE), :])[np.newaxis, :]\n",
    "        \n",
    "        classSum = np.sum(y[i:(i + WINDOW_SIZE)], axis = 0)\n",
    "        maxIdx = np.argmax(classSum)\n",
    "        if classSum[maxIdx] > thresholdWindow:\n",
    "            if not isinstance(retx, np.ndarray):\n",
    "                retx = new.copy()\n",
    "                rety = [maxIdx + 1]\n",
    "            else:\n",
    "                retx = np.concatenate([retx, new], axis=0)\n",
    "                rety.append(maxIdx + 1)\n",
    "        else:\n",
    "            if not isinstance(retUnknown, np.ndarray):\n",
    "                retUnknown = new.copy()\n",
    "            else:\n",
    "                retUnknown = np.concatenate([retUnknown, new], axis=0)\n",
    "\n",
    "        i += SLIDING_STEP\n",
    "        \n",
    "    return np.transpose(retx[:, np.newaxis], (0, 3, 2, 1)), one_hot(rety), np.transpose(retUnknown[:, np.newaxis], (0, 3, 2, 1))\n",
    "\n",
    "def one_hot(arr):\n",
    "    ret = []\n",
    "    for val in arr:\n",
    "        tmp = [0] * CLASS_NUMBER\n",
    "        if val > 0:\n",
    "            tmp[val - 1] = 1\n",
    "        ret.append(np.array(tmp))\n",
    "        \n",
    "    return np.array(ret)\n",
    "\n",
    "def train_test_unknown_split(trainSignal, trainLabel, unknownActions, fold = None,  randomUnknown = True, base = 0.05, rand = 0.1):\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "    for x, y, u in zip(trainSignal, trainLabel, unknownActions):\n",
    "        if (randomUnknown):\n",
    "            if x.shape[0] // CLASS_NUMBER < u.shape[0]:\n",
    "                X_unknown_add = u[np.random.choice(u.shape[0], size = int(x.shape[0] // CLASS_NUMBER), replace = False)]\n",
    "                y_unknown_add = (np.random.rand(int(x.shape[0] // CLASS_NUMBER), CLASS_NUMBER) * rand + base)\n",
    "            else:\n",
    "                X_unknown_add = u\n",
    "                y_unknown_add = (np.random.rand(u.shape[0], CLASS_NUMBER) * rand + base)\n",
    "\n",
    "            Xt = np.concatenate([x, X_unknown_add], axis = 0)\n",
    "            yt = np.concatenate([y, y_unknown_add], axis = 0)\n",
    "            xtr, xte, ytr, yte = train_test_split(Xt, yt, test_size=0.2, random_state=343)\n",
    "        else:\n",
    "            xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.2, random_state=343)\n",
    "            \n",
    "        if not isinstance(X_train, np.ndarray):\n",
    "            X_train = xtr\n",
    "            X_test = xte\n",
    "            y_train = ytr\n",
    "            y_test = yte\n",
    "        else:\n",
    "            X_train = np.concatenate([X_train, xtr], axis=0)\n",
    "            X_test = np.concatenate([X_test, xte], axis=0)\n",
    "            y_train = np.concatenate([y_train, ytr], axis=0)\n",
    "            y_test = np.concatenate([y_test, yte], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate(y, r, belief = BELIEF_THRESHOLD, v = True):\n",
    "    gj, bj, bua = {}, {}, {}\n",
    "    bu, tg, tb = 0, 0, 0\n",
    "\n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        gj[cls] = 0\n",
    "        bj[cls] = 0\n",
    "        \n",
    "    for r, p in zip(y, r.numpy()):\n",
    "        rm = KEY_CLASS[np.argmax(r) + 1 if any(r) else 0]\n",
    "        pm = KEY_CLASS[np.argmax(p) + 1 if p[np.argmax(p)] > belief else 0]\n",
    "        if rm == pm:\n",
    "            tg += 1\n",
    "            gj[rm] += 1\n",
    "        else:\n",
    "            tb += 1\n",
    "            bj[rm] += 1\n",
    "            if rm == \"undefined action\":\n",
    "                bu += 1\n",
    "                if bua.get(pm, None):\n",
    "                    bua[pm] += 1\n",
    "                else:\n",
    "                    bua[pm] = 1\n",
    "            \n",
    "    la, ra, ua = 0, 0, 0\n",
    "    \n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        if v:\n",
    "            print(\"Action: {}, True: {}, False: {}, Accuracy: {:.4f}\".format(cls, gj[cls], bj[cls], gj[cls] / (gj[cls] + bj[cls] + 0.001)))\n",
    "        if cls == \"left\":\n",
    "            la = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "        elif cls == \"right\":\n",
    "            ra = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "        elif cls == \"undefined action\":\n",
    "            ua = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "    if v:\n",
    "        print(\"Left&Right True: {}, False: {}, Accuracy: {:.4f}\".format(gj[\"left\"] + gj[\"right\"], bj[\"left\"] + bj[\"right\"], (gj[\"left\"] + gj[\"right\"]) / (0.001 + gj[\"left\"] + gj[\"right\"] + bj[\"left\"] + bj[\"right\"])))\n",
    "        print(\"Total True: {}, False: {}, Accuracy: {:.4f}\".format(tg, tb, tg / (tg + tb)))\n",
    "        for cls, bp in bua.items():\n",
    "            print(\"Action:{} ,bad prediction times: {}\".format(cls, bp))\n",
    "\n",
    "    return la + ra + 4 * ua\n",
    "\n",
    "def ConTradiction_model(inputShape, d_model = 32, convDropRate = 0.5, encDropRate = 0.7):\n",
    "    input = tf.keras.layers.Input(shape = inputShape)\n",
    "    conv = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 3)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(input)\n",
    "    bnorm = tf.keras.layers.BatchNormalization()(conv)\n",
    "    pooling = tf.keras.layers.AveragePooling2D((1, 8), padding='same')(bnorm)\n",
    "    drop = tf.keras.layers.Dropout(convDropRate)(pooling)\n",
    "    conv2 = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 6)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(drop)\n",
    "    bnorm2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    pooling2 = tf.keras.layers.AveragePooling2D((1, 4), padding='same')(bnorm2)\n",
    "    drop2 = tf.keras.layers.Dropout(convDropRate)(pooling2)\n",
    "\n",
    "    #transformer encoder\n",
    "    encoder = EncoderLayer(d_model, 8, 2 * d_model, encDropRate)(drop2)\n",
    "    #Classification\n",
    "    flatten = tf.keras.layers.Flatten()(encoder)\n",
    "    output = tf.keras.layers.Dense(CLASS_NUMBER, activation='softmax')(flatten)\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lrs(d_model, 50)),\n",
    "                    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preprocess and split record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSignalFiles = glob(\".\\\\data\\\\*_record_X.npy\")\n",
    "trainLabelFiles = [x.replace('X', 'y') for x in trainSignalFiles]\n",
    "\n",
    "numX = 0\n",
    "numUX = 0\n",
    "sigPLot = None\n",
    "trainSignal, trainLabel, unknownActions = [], [], []\n",
    "for sfp, lfp in zip(trainSignalFiles, trainLabelFiles):\n",
    "    print(\"loaded {} and {}.\".format(sfp, lfp))\n",
    "    tempSig = np.load(sfp)[INITIAL_PULSE:]\n",
    "    tempLbl = np.load(lfp)[INITIAL_PULSE:]\n",
    "    sigPLot = tempSig if not isinstance(sigPLot, np.ndarray) else np.concatenate([sigPLot, tempSig], axis=0)\n",
    "    X, y, X_unknown = slicing(tempSig, tempLbl)\n",
    "    trainSignal.append(X)\n",
    "    trainLabel.append(y)\n",
    "    unknownActions.append(X_unknown)\n",
    "    numX += X.shape[0]\n",
    "    numUX += X_unknown.shape[0]\n",
    "\n",
    "print(\"Number of X: {}, unknown X: {}\".format(numX, numUX))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_unknown_split(trainSignal, trainLabel, unknownActions, \n",
    "                                                            randomUnknown = True, base = 0.15, rand = 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1), convDropRate=0.2, encDropRate=0.2)\n",
    "model.summary()\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=200,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=50,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = np.array(history.history['loss']).flatten()\n",
    "valf1 = np.array(history.history['val_loss']).flatten()\n",
    "px.line(pd.DataFrame(np.array([f1, valf1]).T, columns=['loss', 'val_loss'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model/MTJaw0402_250_W250_T8529/\", save_format=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unknown = None\n",
    "for u in unknownActions:\n",
    "    if not isinstance(X_unknown, np.ndarray):\n",
    "        X_unknown = u\n",
    "    else:\n",
    "        X_unknown = np.concatenate([X_unknown, u], axis=0)\n",
    "All_X = np.concatenate([X_test, X_unknown])\n",
    "All_y = np.concatenate([y_test, np.array([[0] * CLASS_NUMBER for _ in range(X_unknown.shape[0])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(All_X)\n",
    "evaluate(All_y, res, belief = 0.8629)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search\n",
    "parameterSelection = []\n",
    "lb, ub = 0, 0.9999\n",
    "best = None\n",
    "for n in range(3):\n",
    "    for i in range(10):\n",
    "        threshold = lb + (i + 1) * 0.1 * (ub - lb)\n",
    "        parameterSelection.append([threshold, evaluate(All_y, res, belief = threshold, v = False)])\n",
    "    parameterSelection.sort(key=lambda x:x[1])\n",
    "    lb, ub = parameterSelection[-2][0], parameterSelection[-1][0]\n",
    "    best = parameterSelection[-1]\n",
    "    print(\"iteration\", n, \":\", best, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1))\n",
    "test.load_weights('./model/' + \"MTJaw0401_1300_T8969\" + '/')\n",
    "# res = test(All_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_29_203224_l5m6r7_record_X.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_29_203224_l5m6r7_record_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, u = slicing(np.load('./data/2023_Mar_29_203224_l5m6r7_record_X.npy'), np.load('./data/2023_Mar_29_203224_l5m6r7_record_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(np.concatenate([X, u]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(np.concatenate([y, np.zeros((u.shape[0], CLASS_NUMBER))]), r, belief = 0.8969)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

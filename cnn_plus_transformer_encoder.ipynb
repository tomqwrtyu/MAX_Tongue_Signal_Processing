{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package & Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "# import emd\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL_NUMBER = 3\n",
    "WINDOW_SIZE = 200\n",
    "SLIDING_STEP = int(WINDOW_SIZE * 0.25)\n",
    "KEY_CLASS = {0:'undefined action', 1:'up', 2:'down', 3:'left', 4:'right', 5:'quick touch'}\n",
    "CLASS_NUMBER = 5 # 0 is not a class\n",
    "NUM_IMF = 3\n",
    "LABEL_THRESHOLD = 0.8\n",
    "BELIEF_THRESHOLD = 0.8\n",
    "INITIAL_PULSE = 100 # abandon initial pulse data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture related definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size = 1024, d_model = 32):\n",
    "        super().__init__()\n",
    "        def positional_encoding(length, depth):\n",
    "            depth = depth/2\n",
    "\n",
    "            positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "            depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "            angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "            angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "            pos_encoding = np.concatenate(\n",
    "                [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "                axis=-1) \n",
    "\n",
    "            return pos_encoding\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(2048, d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.image.extract_patches(images=x,\n",
    "                                    sizes=[1, CHANNEL_NUMBER, 2, x.shape[-1]],\n",
    "                                    strides=[1, CHANNEL_NUMBER, 1, x.shape[-1]],\n",
    "                                    rates=[1, 1, 1, 1],\n",
    "                                    padding='VALID')\n",
    "        patch_dims = x.shape[-1]\n",
    "        x = tf.reshape(x, [batch_size, x.shape[1] * x.shape[2], patch_dims])\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float16))\n",
    "        pe = self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]\n",
    "        for _ in range(x.shape[1] - 1):\n",
    "            pe = np.concatenate([pe, self.pos_encoding[np.newaxis, np.newaxis, :patch_dims, :]], axis=1)\n",
    "        x = x + tf.cast(pe, dtype=tf.float16)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        def point_wise_feed_forward_network(d_model, dff):\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(dff, activation='elu'),  # (batch_size, seq_len, dff)\n",
    "                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "            ])\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = d_model)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        attn_output = self.mha(x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class lrs(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicing(x, y):\n",
    "    totalLength = x.shape[0]\n",
    "    assert totalLength == y.shape[0], \"Data numbers not matching with that of labels.\"\n",
    "    if totalLength <= WINDOW_SIZE:\n",
    "        return x, y\n",
    "\n",
    "    y = one_hot(y)\n",
    "    \n",
    "    thresholdWindow = LABEL_THRESHOLD * WINDOW_SIZE\n",
    "    retx = None\n",
    "    rety = None\n",
    "    retUnknown = None\n",
    "    \n",
    "    i = 0\n",
    "    while (totalLength - i) > WINDOW_SIZE:\n",
    "        new = (x[i:(i + WINDOW_SIZE), :])[np.newaxis, :]\n",
    "        \n",
    "        classSum = np.sum(y[i:(i + WINDOW_SIZE)], axis = 0)\n",
    "        maxIdx = np.argmax(classSum)\n",
    "        if classSum[maxIdx] > thresholdWindow:\n",
    "            if not isinstance(retx, np.ndarray):\n",
    "                retx = new.copy()\n",
    "                rety = [maxIdx + 1]\n",
    "            else:\n",
    "                retx = np.concatenate([retx, new], axis=0)\n",
    "                rety.append(maxIdx + 1)\n",
    "        else:\n",
    "            if not isinstance(retUnknown, np.ndarray):\n",
    "                retUnknown = new.copy()\n",
    "            else:\n",
    "                retUnknown = np.concatenate([retUnknown, new], axis=0)\n",
    "\n",
    "        i += SLIDING_STEP\n",
    "        \n",
    "    return np.transpose(retx[:, np.newaxis], (0, 3, 2, 1)), one_hot(rety), np.transpose(retUnknown[:, np.newaxis], (0, 3, 2, 1))\n",
    "\n",
    "def one_hot(arr):\n",
    "    ret = []\n",
    "    for val in arr:\n",
    "        tmp = [0] * CLASS_NUMBER\n",
    "        if val > 0:\n",
    "            tmp[val - 1] = 1\n",
    "        ret.append(np.array(tmp))\n",
    "        \n",
    "    return np.array(ret)\n",
    "\n",
    "def train_test_unknown_split(trainSignal, trainLabel, unknownActions, seed = 343, fold = None,  randomUnknown = True, base = 0.05, rand = 0.1):\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "    for x, y, u in zip(trainSignal, trainLabel, unknownActions):\n",
    "        if (randomUnknown):\n",
    "            if x.shape[0] // CLASS_NUMBER < u.shape[0]:\n",
    "                X_unknown_add = u[np.random.choice(u.shape[0], size = int(x.shape[0] // CLASS_NUMBER), replace = False)]\n",
    "                y_unknown_add = (np.random.rand(int(x.shape[0] // CLASS_NUMBER), CLASS_NUMBER) * rand + base)\n",
    "            else:\n",
    "                X_unknown_add = u\n",
    "                y_unknown_add = (np.random.rand(u.shape[0], CLASS_NUMBER) * rand + base)\n",
    "\n",
    "            Xt = np.concatenate([x, X_unknown_add], axis = 0)\n",
    "            yt = np.concatenate([y, y_unknown_add], axis = 0)\n",
    "            xtr, xte, ytr, yte = train_test_split(Xt, yt, test_size=0.2, random_state=seed)\n",
    "        else:\n",
    "            xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "            \n",
    "        if not isinstance(X_train, np.ndarray):\n",
    "            X_train = xtr\n",
    "            X_test = xte\n",
    "            y_train = ytr\n",
    "            y_test = yte\n",
    "        else:\n",
    "            X_train = np.concatenate([X_train, xtr], axis=0)\n",
    "            X_test = np.concatenate([X_test, xte], axis=0)\n",
    "            y_train = np.concatenate([y_train, ytr], axis=0)\n",
    "            y_test = np.concatenate([y_test, yte], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate(y, r, uw = 3, belief = BELIEF_THRESHOLD, v = True):\n",
    "    gj, bj, bua = {}, {}, {}\n",
    "    bu, tg, tb, lrconfusion, rlconfusion = 0, 0, 0, 0, 0\n",
    "\n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        gj[cls] = 0\n",
    "        bj[cls] = 0\n",
    "        \n",
    "    for r, p in zip(y, r.numpy()):\n",
    "        rm = KEY_CLASS[np.argmax(r) + 1 if any(r) else 0]\n",
    "        pm = KEY_CLASS[np.argmax(p) + 1 if p[np.argmax(p)] > belief else 0]\n",
    "        if rm == pm:\n",
    "            tg += 1\n",
    "            gj[rm] += 1\n",
    "        else:\n",
    "            tb += 1\n",
    "            bj[rm] += 1\n",
    "            if rm == \"undefined action\":\n",
    "                bu += 1\n",
    "                if bua.get(pm, None):\n",
    "                    bua[pm] += 1\n",
    "                else:\n",
    "                    bua[pm] = 1\n",
    "            elif (rm == \"left\") and (pm == \"right\"):\n",
    "                lrconfusion += 1\n",
    "            elif (rm == \"right\") and (pm == \"left\"):\n",
    "                rlconfusion += 1\n",
    "\n",
    "    la, ra, ua = 0, 0, 0\n",
    "    \n",
    "    for key, cls in KEY_CLASS.items():\n",
    "        if v:\n",
    "            print(\"Action: {}, True: {}, False: {}, Accuracy: {:.4f}\".format(cls, gj[cls], bj[cls], gj[cls] / (gj[cls] + bj[cls] + 0.001)))\n",
    "        if cls == \"left\":\n",
    "            la = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "            if v:\n",
    "                print(\"Massive prediction error times: {}, portion: {:.4f}.\".format(lrconfusion, lrconfusion / bj[cls]))\n",
    "        elif cls == \"right\":\n",
    "            ra = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "            if v:\n",
    "                print(\"Massive prediction error times: {}, portion: {:.4f}.\".format(rlconfusion, rlconfusion / bj[cls]))\n",
    "        elif cls == \"undefined action\":\n",
    "            ua = gj[cls] / (gj[cls] + bj[cls] + 0.001)\n",
    "    if v:\n",
    "        print(\"Total True: {}, False: {}, Accuracy: {:.4f}\".format(tg, tb, tg / (tg + tb)))\n",
    "        for cls, bp in bua.items():\n",
    "            print(\"Action:{} ,bad prediction times: {}\".format(cls, bp))\n",
    "\n",
    "    return la + ra + uw * ua\n",
    "\n",
    "def ConTradiction_model(inputShape, d_model = 32, convDropRate = 0.5, encDropRate = 0.7):\n",
    "    input = tf.keras.layers.Input(shape = inputShape)\n",
    "    conv = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 3)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(input)\n",
    "    bnorm = tf.keras.layers.BatchNormalization()(conv)\n",
    "    pooling = tf.keras.layers.AveragePooling2D((1, 8), padding='same')(bnorm)\n",
    "    drop = tf.keras.layers.Dropout(convDropRate)(pooling)\n",
    "    conv2 = tf.keras.layers.Conv2D(d_model, (1, int(WINDOW_SIZE * 0.5 // 6)), padding='same', activation='elu',\n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(0.25))(drop)\n",
    "    bnorm2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    pooling2 = tf.keras.layers.AveragePooling2D((1, 4), padding='same')(bnorm2)\n",
    "    drop2 = tf.keras.layers.Dropout(convDropRate)(pooling2)\n",
    "\n",
    "    #transformer encoder\n",
    "    encoder = EncoderLayer(d_model, 8, 2 * d_model, encDropRate)(drop2)\n",
    "    #Classification\n",
    "    flatten = tf.keras.layers.Flatten()(encoder)\n",
    "    output = tf.keras.layers.Dense(CLASS_NUMBER, activation='softmax')(flatten)\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lrs(d_model, 50)),\n",
    "                    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preprocess and split record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded .\\data\\2023_Apr_11_145938_l5m6r7_record_X.npy and .\\data\\2023_Apr_11_145938_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_11_224807_l5m6r7_record_X.npy and .\\data\\2023_Mar_11_224807_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_12_015657_l5m6r7_record_X.npy and .\\data\\2023_Mar_12_015657_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_14_153445_l5m6r7_record_X.npy and .\\data\\2023_Mar_14_153445_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_14_170710_l5m6r7_record_X.npy and .\\data\\2023_Mar_14_170710_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_21_182116_l5m6r7_record_X.npy and .\\data\\2023_Mar_21_182116_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_29_191038_l5m6r7_record_X.npy and .\\data\\2023_Mar_29_191038_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_29_203224_l5m6r7_record_X.npy and .\\data\\2023_Mar_29_203224_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_165712_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_165712_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_174506_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_174506_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_184318_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_184318_l5m6r7_record_y.npy.\n",
      "loaded .\\data\\2023_Mar_30_184837_l5m6r7_record_X.npy and .\\data\\2023_Mar_30_184837_l5m6r7_record_y.npy.\n",
      "Number of X: 5057, unknown X: 6246\n"
     ]
    }
   ],
   "source": [
    "trainSignalFiles = glob(\".\\\\data\\\\*_record_X.npy\")\n",
    "trainLabelFiles = [x.replace('X', 'y') for x in trainSignalFiles]\n",
    "\n",
    "numX = 0\n",
    "numUX = 0\n",
    "sigPLot = None\n",
    "trainSignal, trainLabel, unknownActions = [], [], []\n",
    "for sfp, lfp in zip(trainSignalFiles, trainLabelFiles):\n",
    "    print(\"loaded {} and {}.\".format(sfp, lfp))\n",
    "    tempSig = np.load(sfp)[INITIAL_PULSE:]\n",
    "    tempLbl = np.load(lfp)[INITIAL_PULSE:]\n",
    "    sigPLot = tempSig if not isinstance(sigPLot, np.ndarray) else np.concatenate([sigPLot, tempSig], axis=0)\n",
    "    X, y, X_unknown = slicing(tempSig, tempLbl)\n",
    "    trainSignal.append(X)\n",
    "    trainLabel.append(y)\n",
    "    unknownActions.append(X_unknown)\n",
    "    numX += X.shape[0]\n",
    "    numUX += X_unknown.shape[0]\n",
    "\n",
    "print(\"Number of X: {}, unknown X: {}\".format(numX, numUX))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_unknown_split(trainSignal, trainLabel, unknownActions, seed = 343,\n",
    "                                                            randomUnknown = True, base = 0.15, rand = 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3, 200, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 200, 32)        1088      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 200, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 3, 25, 32)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 3, 25, 32)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 25, 32)         2080      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 3, 25, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 3, 7, 32)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 3, 7, 32)          0         \n",
      "                                                                 \n",
      " encoder_layer_1 (EncoderLay  (None, 3, 7, 32)         37888     \n",
      " er)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 672)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 3365      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,677\n",
      "Trainable params: 44,549\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "152/152 [==============================] - 3s 10ms/step - loss: 1.4132 - categorical_accuracy: 0.4132 - val_loss: 1.2125 - val_categorical_accuracy: 0.4289\n",
      "Epoch 2/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.1484 - categorical_accuracy: 0.4855 - val_loss: 1.2929 - val_categorical_accuracy: 0.4396\n",
      "Epoch 3/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.1241 - categorical_accuracy: 0.4912 - val_loss: 1.1277 - val_categorical_accuracy: 0.4766\n",
      "Epoch 4/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.1147 - categorical_accuracy: 0.4991 - val_loss: 1.1240 - val_categorical_accuracy: 0.4963\n",
      "Epoch 5/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0926 - categorical_accuracy: 0.5123 - val_loss: 1.0786 - val_categorical_accuracy: 0.5127\n",
      "Epoch 6/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0856 - categorical_accuracy: 0.5168 - val_loss: 1.1399 - val_categorical_accuracy: 0.5177\n",
      "Epoch 7/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0875 - categorical_accuracy: 0.5110 - val_loss: 1.0716 - val_categorical_accuracy: 0.5119\n",
      "Epoch 8/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 1.0779 - categorical_accuracy: 0.5224 - val_loss: 1.0633 - val_categorical_accuracy: 0.5316\n",
      "Epoch 9/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0633 - categorical_accuracy: 0.5265 - val_loss: 1.1005 - val_categorical_accuracy: 0.5094\n",
      "Epoch 10/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0635 - categorical_accuracy: 0.5263 - val_loss: 1.0479 - val_categorical_accuracy: 0.5366\n",
      "Epoch 11/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0675 - categorical_accuracy: 0.5280 - val_loss: 1.0429 - val_categorical_accuracy: 0.5415\n",
      "Epoch 12/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0592 - categorical_accuracy: 0.5306 - val_loss: 1.0896 - val_categorical_accuracy: 0.5596\n",
      "Epoch 13/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0556 - categorical_accuracy: 0.5346 - val_loss: 1.0713 - val_categorical_accuracy: 0.5251\n",
      "Epoch 14/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0513 - categorical_accuracy: 0.5321 - val_loss: 1.0375 - val_categorical_accuracy: 0.5555\n",
      "Epoch 15/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0545 - categorical_accuracy: 0.5418 - val_loss: 1.0751 - val_categorical_accuracy: 0.5275\n",
      "Epoch 16/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0503 - categorical_accuracy: 0.5416 - val_loss: 1.0758 - val_categorical_accuracy: 0.5407\n",
      "Epoch 17/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0412 - categorical_accuracy: 0.5430 - val_loss: 1.0570 - val_categorical_accuracy: 0.5514\n",
      "Epoch 18/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0434 - categorical_accuracy: 0.5436 - val_loss: 1.0624 - val_categorical_accuracy: 0.5283\n",
      "Epoch 19/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0431 - categorical_accuracy: 0.5403 - val_loss: 1.0554 - val_categorical_accuracy: 0.5530\n",
      "Epoch 20/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0426 - categorical_accuracy: 0.5443 - val_loss: 1.0252 - val_categorical_accuracy: 0.5390\n",
      "Epoch 21/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0273 - categorical_accuracy: 0.5593 - val_loss: 1.0649 - val_categorical_accuracy: 0.5448\n",
      "Epoch 22/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0367 - categorical_accuracy: 0.5573 - val_loss: 1.0531 - val_categorical_accuracy: 0.5325\n",
      "Epoch 23/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0296 - categorical_accuracy: 0.5515 - val_loss: 1.0487 - val_categorical_accuracy: 0.5514\n",
      "Epoch 24/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0247 - categorical_accuracy: 0.5521 - val_loss: 1.0520 - val_categorical_accuracy: 0.5382\n",
      "Epoch 25/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0253 - categorical_accuracy: 0.5665 - val_loss: 1.0372 - val_categorical_accuracy: 0.5497\n",
      "Epoch 26/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0253 - categorical_accuracy: 0.5558 - val_loss: 1.0526 - val_categorical_accuracy: 0.5472\n",
      "Epoch 27/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0202 - categorical_accuracy: 0.5686 - val_loss: 1.0398 - val_categorical_accuracy: 0.5464\n",
      "Epoch 28/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0218 - categorical_accuracy: 0.5750 - val_loss: 1.0329 - val_categorical_accuracy: 0.5530\n",
      "Epoch 29/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0115 - categorical_accuracy: 0.5641 - val_loss: 1.0688 - val_categorical_accuracy: 0.5456\n",
      "Epoch 30/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0109 - categorical_accuracy: 0.5603 - val_loss: 1.0501 - val_categorical_accuracy: 0.5612\n",
      "Epoch 31/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0102 - categorical_accuracy: 0.5746 - val_loss: 1.0439 - val_categorical_accuracy: 0.5333\n",
      "Epoch 32/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0228 - categorical_accuracy: 0.5636 - val_loss: 1.0241 - val_categorical_accuracy: 0.5678\n",
      "Epoch 33/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0184 - categorical_accuracy: 0.5723 - val_loss: 1.0320 - val_categorical_accuracy: 0.5768\n",
      "Epoch 34/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0090 - categorical_accuracy: 0.5698 - val_loss: 1.0600 - val_categorical_accuracy: 0.5522\n",
      "Epoch 35/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0071 - categorical_accuracy: 0.5748 - val_loss: 1.0464 - val_categorical_accuracy: 0.5629\n",
      "Epoch 36/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0061 - categorical_accuracy: 0.5810 - val_loss: 1.0290 - val_categorical_accuracy: 0.5579\n",
      "Epoch 37/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0009 - categorical_accuracy: 0.5721 - val_loss: 1.0265 - val_categorical_accuracy: 0.5620\n",
      "Epoch 38/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 1.0002 - categorical_accuracy: 0.5779 - val_loss: 1.0157 - val_categorical_accuracy: 0.5924\n",
      "Epoch 39/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0084 - categorical_accuracy: 0.5700 - val_loss: 1.0096 - val_categorical_accuracy: 0.5768\n",
      "Epoch 40/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9957 - categorical_accuracy: 0.5802 - val_loss: 1.0108 - val_categorical_accuracy: 0.5744\n",
      "Epoch 41/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 1.0070 - categorical_accuracy: 0.5657 - val_loss: 1.0348 - val_categorical_accuracy: 0.5670\n",
      "Epoch 42/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9933 - categorical_accuracy: 0.5802 - val_loss: 0.9943 - val_categorical_accuracy: 0.5867\n",
      "Epoch 43/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9932 - categorical_accuracy: 0.5820 - val_loss: 1.0446 - val_categorical_accuracy: 0.5645\n",
      "Epoch 44/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9997 - categorical_accuracy: 0.5828 - val_loss: 1.0378 - val_categorical_accuracy: 0.5727\n",
      "Epoch 45/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9871 - categorical_accuracy: 0.5824 - val_loss: 0.9880 - val_categorical_accuracy: 0.5933\n",
      "Epoch 46/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9880 - categorical_accuracy: 0.5849 - val_loss: 1.0497 - val_categorical_accuracy: 0.5637\n",
      "Epoch 47/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9871 - categorical_accuracy: 0.5896 - val_loss: 1.0257 - val_categorical_accuracy: 0.5563\n",
      "Epoch 48/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9813 - categorical_accuracy: 0.5861 - val_loss: 1.0418 - val_categorical_accuracy: 0.5440\n",
      "Epoch 49/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9881 - categorical_accuracy: 0.5890 - val_loss: 0.9872 - val_categorical_accuracy: 0.5892\n",
      "Epoch 50/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9817 - categorical_accuracy: 0.5901 - val_loss: 1.0017 - val_categorical_accuracy: 0.5793\n",
      "Epoch 51/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9807 - categorical_accuracy: 0.5874 - val_loss: 1.0237 - val_categorical_accuracy: 0.5826\n",
      "Epoch 52/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9838 - categorical_accuracy: 0.5832 - val_loss: 1.0270 - val_categorical_accuracy: 0.5793\n",
      "Epoch 53/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9734 - categorical_accuracy: 0.5969 - val_loss: 1.0326 - val_categorical_accuracy: 0.5727\n",
      "Epoch 54/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9839 - categorical_accuracy: 0.5791 - val_loss: 1.0483 - val_categorical_accuracy: 0.5826\n",
      "Epoch 55/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9785 - categorical_accuracy: 0.5882 - val_loss: 1.0184 - val_categorical_accuracy: 0.5850\n",
      "Epoch 56/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9689 - categorical_accuracy: 0.5946 - val_loss: 1.0298 - val_categorical_accuracy: 0.6007\n",
      "Epoch 57/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9664 - categorical_accuracy: 0.5956 - val_loss: 1.0442 - val_categorical_accuracy: 0.5957\n",
      "Epoch 58/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9696 - categorical_accuracy: 0.5969 - val_loss: 1.0042 - val_categorical_accuracy: 0.6023\n",
      "Epoch 59/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9721 - categorical_accuracy: 0.5853 - val_loss: 1.0174 - val_categorical_accuracy: 0.5949\n",
      "Epoch 60/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9641 - categorical_accuracy: 0.5896 - val_loss: 1.0420 - val_categorical_accuracy: 0.5859\n",
      "Epoch 61/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9640 - categorical_accuracy: 0.5948 - val_loss: 1.0112 - val_categorical_accuracy: 0.5941\n",
      "Epoch 62/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9581 - categorical_accuracy: 0.6010 - val_loss: 1.0832 - val_categorical_accuracy: 0.5965\n",
      "Epoch 63/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9565 - categorical_accuracy: 0.6074 - val_loss: 1.0148 - val_categorical_accuracy: 0.6081\n",
      "Epoch 64/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9604 - categorical_accuracy: 0.5907 - val_loss: 0.9781 - val_categorical_accuracy: 0.6072\n",
      "Epoch 65/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9590 - categorical_accuracy: 0.6057 - val_loss: 0.9790 - val_categorical_accuracy: 0.6081\n",
      "Epoch 66/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9563 - categorical_accuracy: 0.6055 - val_loss: 1.0561 - val_categorical_accuracy: 0.5760\n",
      "Epoch 67/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9564 - categorical_accuracy: 0.5934 - val_loss: 1.0457 - val_categorical_accuracy: 0.5809\n",
      "Epoch 68/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9477 - categorical_accuracy: 0.6115 - val_loss: 0.9588 - val_categorical_accuracy: 0.6056\n",
      "Epoch 69/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9510 - categorical_accuracy: 0.6035 - val_loss: 0.9911 - val_categorical_accuracy: 0.5998\n",
      "Epoch 70/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9511 - categorical_accuracy: 0.6016 - val_loss: 1.0049 - val_categorical_accuracy: 0.5982\n",
      "Epoch 71/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9555 - categorical_accuracy: 0.6076 - val_loss: 1.0024 - val_categorical_accuracy: 0.5908\n",
      "Epoch 72/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9480 - categorical_accuracy: 0.6037 - val_loss: 0.9749 - val_categorical_accuracy: 0.5974\n",
      "Epoch 73/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9567 - categorical_accuracy: 0.5989 - val_loss: 0.9595 - val_categorical_accuracy: 0.6097\n",
      "Epoch 74/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9405 - categorical_accuracy: 0.6037 - val_loss: 0.9611 - val_categorical_accuracy: 0.6138\n",
      "Epoch 75/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9495 - categorical_accuracy: 0.6002 - val_loss: 0.9649 - val_categorical_accuracy: 0.6204\n",
      "Epoch 76/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9405 - categorical_accuracy: 0.6059 - val_loss: 1.0093 - val_categorical_accuracy: 0.6007\n",
      "Epoch 77/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9522 - categorical_accuracy: 0.6072 - val_loss: 0.9875 - val_categorical_accuracy: 0.5965\n",
      "Epoch 78/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9417 - categorical_accuracy: 0.6105 - val_loss: 0.9922 - val_categorical_accuracy: 0.6253\n",
      "Epoch 79/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9386 - categorical_accuracy: 0.6161 - val_loss: 1.0000 - val_categorical_accuracy: 0.5949\n",
      "Epoch 80/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9362 - categorical_accuracy: 0.6128 - val_loss: 0.9682 - val_categorical_accuracy: 0.6007\n",
      "Epoch 81/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9335 - categorical_accuracy: 0.6212 - val_loss: 0.9653 - val_categorical_accuracy: 0.6097\n",
      "Epoch 82/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9304 - categorical_accuracy: 0.6130 - val_loss: 0.9528 - val_categorical_accuracy: 0.6138\n",
      "Epoch 83/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9350 - categorical_accuracy: 0.6196 - val_loss: 0.9892 - val_categorical_accuracy: 0.6138\n",
      "Epoch 84/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9354 - categorical_accuracy: 0.6187 - val_loss: 0.9722 - val_categorical_accuracy: 0.6130\n",
      "Epoch 85/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9260 - categorical_accuracy: 0.6169 - val_loss: 0.9762 - val_categorical_accuracy: 0.6196\n",
      "Epoch 86/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9257 - categorical_accuracy: 0.6235 - val_loss: 0.9356 - val_categorical_accuracy: 0.6261\n",
      "Epoch 87/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9171 - categorical_accuracy: 0.6260 - val_loss: 0.9809 - val_categorical_accuracy: 0.6007\n",
      "Epoch 88/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9285 - categorical_accuracy: 0.6183 - val_loss: 0.9846 - val_categorical_accuracy: 0.5965\n",
      "Epoch 89/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9193 - categorical_accuracy: 0.6144 - val_loss: 0.9760 - val_categorical_accuracy: 0.6220\n",
      "Epoch 90/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9190 - categorical_accuracy: 0.6227 - val_loss: 0.9510 - val_categorical_accuracy: 0.6130\n",
      "Epoch 91/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9107 - categorical_accuracy: 0.6295 - val_loss: 0.9843 - val_categorical_accuracy: 0.6089\n",
      "Epoch 92/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9218 - categorical_accuracy: 0.6210 - val_loss: 1.0179 - val_categorical_accuracy: 0.6163\n",
      "Epoch 93/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9235 - categorical_accuracy: 0.6206 - val_loss: 0.9669 - val_categorical_accuracy: 0.6343\n",
      "Epoch 94/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9177 - categorical_accuracy: 0.6214 - val_loss: 0.9391 - val_categorical_accuracy: 0.6286\n",
      "Epoch 95/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9127 - categorical_accuracy: 0.6286 - val_loss: 0.9533 - val_categorical_accuracy: 0.6302\n",
      "Epoch 96/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9210 - categorical_accuracy: 0.6152 - val_loss: 0.9783 - val_categorical_accuracy: 0.6023\n",
      "Epoch 97/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9184 - categorical_accuracy: 0.6173 - val_loss: 0.9396 - val_categorical_accuracy: 0.6294\n",
      "Epoch 98/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9062 - categorical_accuracy: 0.6336 - val_loss: 0.9324 - val_categorical_accuracy: 0.6278\n",
      "Epoch 99/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9139 - categorical_accuracy: 0.6288 - val_loss: 0.9830 - val_categorical_accuracy: 0.6237\n",
      "Epoch 100/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9125 - categorical_accuracy: 0.6251 - val_loss: 0.9545 - val_categorical_accuracy: 0.6311\n",
      "Epoch 101/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9119 - categorical_accuracy: 0.6311 - val_loss: 1.0145 - val_categorical_accuracy: 0.6048\n",
      "Epoch 102/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9202 - categorical_accuracy: 0.6196 - val_loss: 0.9359 - val_categorical_accuracy: 0.6393\n",
      "Epoch 103/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9049 - categorical_accuracy: 0.6392 - val_loss: 0.9215 - val_categorical_accuracy: 0.6360\n",
      "Epoch 104/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9170 - categorical_accuracy: 0.6249 - val_loss: 0.9247 - val_categorical_accuracy: 0.6360\n",
      "Epoch 105/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9122 - categorical_accuracy: 0.6303 - val_loss: 0.9393 - val_categorical_accuracy: 0.6294\n",
      "Epoch 106/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9040 - categorical_accuracy: 0.6276 - val_loss: 0.8973 - val_categorical_accuracy: 0.6426\n",
      "Epoch 107/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9039 - categorical_accuracy: 0.6297 - val_loss: 0.9646 - val_categorical_accuracy: 0.6187\n",
      "Epoch 108/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8896 - categorical_accuracy: 0.6414 - val_loss: 1.0055 - val_categorical_accuracy: 0.6130\n",
      "Epoch 109/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8897 - categorical_accuracy: 0.6282 - val_loss: 0.9762 - val_categorical_accuracy: 0.6146\n",
      "Epoch 110/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9032 - categorical_accuracy: 0.6367 - val_loss: 0.9354 - val_categorical_accuracy: 0.6212\n",
      "Epoch 111/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8920 - categorical_accuracy: 0.6396 - val_loss: 0.9432 - val_categorical_accuracy: 0.6385\n",
      "Epoch 112/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8984 - categorical_accuracy: 0.6293 - val_loss: 0.9024 - val_categorical_accuracy: 0.6343\n",
      "Epoch 113/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8909 - categorical_accuracy: 0.6449 - val_loss: 0.9586 - val_categorical_accuracy: 0.6360\n",
      "Epoch 114/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8958 - categorical_accuracy: 0.6332 - val_loss: 0.9241 - val_categorical_accuracy: 0.6352\n",
      "Epoch 115/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8851 - categorical_accuracy: 0.6387 - val_loss: 0.9363 - val_categorical_accuracy: 0.6327\n",
      "Epoch 116/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8935 - categorical_accuracy: 0.6435 - val_loss: 0.8898 - val_categorical_accuracy: 0.6549\n",
      "Epoch 117/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8979 - categorical_accuracy: 0.6326 - val_loss: 0.9137 - val_categorical_accuracy: 0.6368\n",
      "Epoch 118/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8903 - categorical_accuracy: 0.6441 - val_loss: 0.9559 - val_categorical_accuracy: 0.6253\n",
      "Epoch 119/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8832 - categorical_accuracy: 0.6474 - val_loss: 0.9495 - val_categorical_accuracy: 0.6409\n",
      "Epoch 120/500\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.8975 - categorical_accuracy: 0.6396 - val_loss: 0.9185 - val_categorical_accuracy: 0.6327\n",
      "Epoch 121/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8936 - categorical_accuracy: 0.6385 - val_loss: 0.9025 - val_categorical_accuracy: 0.6442\n",
      "Epoch 122/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8765 - categorical_accuracy: 0.6491 - val_loss: 0.9248 - val_categorical_accuracy: 0.6376\n",
      "Epoch 123/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8918 - categorical_accuracy: 0.6338 - val_loss: 0.9199 - val_categorical_accuracy: 0.6352\n",
      "Epoch 124/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8806 - categorical_accuracy: 0.6484 - val_loss: 0.9036 - val_categorical_accuracy: 0.6409\n",
      "Epoch 125/500\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.8828 - categorical_accuracy: 0.6394 - val_loss: 0.9166 - val_categorical_accuracy: 0.6393\n",
      "Epoch 126/500\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.8778 - categorical_accuracy: 0.6408 - val_loss: 0.9367 - val_categorical_accuracy: 0.6278\n",
      "Epoch 127/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8861 - categorical_accuracy: 0.6394 - val_loss: 0.9285 - val_categorical_accuracy: 0.6385\n",
      "Epoch 128/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8769 - categorical_accuracy: 0.6493 - val_loss: 0.9187 - val_categorical_accuracy: 0.6228\n",
      "Epoch 129/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8734 - categorical_accuracy: 0.6431 - val_loss: 0.9521 - val_categorical_accuracy: 0.6327\n",
      "Epoch 130/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8747 - categorical_accuracy: 0.6408 - val_loss: 0.9138 - val_categorical_accuracy: 0.6385\n",
      "Epoch 131/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8808 - categorical_accuracy: 0.6404 - val_loss: 0.9047 - val_categorical_accuracy: 0.6368\n",
      "Epoch 132/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8826 - categorical_accuracy: 0.6402 - val_loss: 0.8899 - val_categorical_accuracy: 0.6294\n",
      "Epoch 133/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8659 - categorical_accuracy: 0.6480 - val_loss: 0.9275 - val_categorical_accuracy: 0.6187\n",
      "Epoch 134/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8765 - categorical_accuracy: 0.6456 - val_loss: 0.8945 - val_categorical_accuracy: 0.6401\n",
      "Epoch 135/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8795 - categorical_accuracy: 0.6462 - val_loss: 0.9046 - val_categorical_accuracy: 0.6491\n",
      "Epoch 136/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8685 - categorical_accuracy: 0.6499 - val_loss: 0.9332 - val_categorical_accuracy: 0.6245\n",
      "Epoch 137/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8669 - categorical_accuracy: 0.6466 - val_loss: 0.9231 - val_categorical_accuracy: 0.6270\n",
      "Epoch 138/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8744 - categorical_accuracy: 0.6476 - val_loss: 0.9129 - val_categorical_accuracy: 0.6426\n",
      "Epoch 139/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8676 - categorical_accuracy: 0.6466 - val_loss: 0.9182 - val_categorical_accuracy: 0.6368\n",
      "Epoch 140/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8659 - categorical_accuracy: 0.6497 - val_loss: 0.9048 - val_categorical_accuracy: 0.6302\n",
      "Epoch 141/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8610 - categorical_accuracy: 0.6503 - val_loss: 0.8909 - val_categorical_accuracy: 0.6376\n",
      "Epoch 142/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8581 - categorical_accuracy: 0.6583 - val_loss: 0.8944 - val_categorical_accuracy: 0.6360\n",
      "Epoch 143/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8533 - categorical_accuracy: 0.6453 - val_loss: 0.8950 - val_categorical_accuracy: 0.6434\n",
      "Epoch 144/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8585 - categorical_accuracy: 0.6557 - val_loss: 0.8816 - val_categorical_accuracy: 0.6574\n",
      "Epoch 145/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8706 - categorical_accuracy: 0.6476 - val_loss: 0.8768 - val_categorical_accuracy: 0.6549\n",
      "Epoch 146/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8661 - categorical_accuracy: 0.6495 - val_loss: 0.8992 - val_categorical_accuracy: 0.6311\n",
      "Epoch 147/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8554 - categorical_accuracy: 0.6606 - val_loss: 0.8983 - val_categorical_accuracy: 0.6450\n",
      "Epoch 148/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8559 - categorical_accuracy: 0.6555 - val_loss: 0.8740 - val_categorical_accuracy: 0.6549\n",
      "Epoch 149/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8629 - categorical_accuracy: 0.6460 - val_loss: 0.8963 - val_categorical_accuracy: 0.6442\n",
      "Epoch 150/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8548 - categorical_accuracy: 0.6569 - val_loss: 0.8835 - val_categorical_accuracy: 0.6541\n",
      "Epoch 151/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8579 - categorical_accuracy: 0.6586 - val_loss: 0.9120 - val_categorical_accuracy: 0.6500\n",
      "Epoch 152/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8486 - categorical_accuracy: 0.6654 - val_loss: 0.9111 - val_categorical_accuracy: 0.6327\n",
      "Epoch 153/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8596 - categorical_accuracy: 0.6583 - val_loss: 0.8856 - val_categorical_accuracy: 0.6491\n",
      "Epoch 154/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8552 - categorical_accuracy: 0.6538 - val_loss: 0.8846 - val_categorical_accuracy: 0.6590\n",
      "Epoch 155/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8530 - categorical_accuracy: 0.6647 - val_loss: 0.8654 - val_categorical_accuracy: 0.6582\n",
      "Epoch 156/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8521 - categorical_accuracy: 0.6600 - val_loss: 0.8596 - val_categorical_accuracy: 0.6656\n",
      "Epoch 157/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8513 - categorical_accuracy: 0.6559 - val_loss: 0.9004 - val_categorical_accuracy: 0.6459\n",
      "Epoch 158/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8515 - categorical_accuracy: 0.6637 - val_loss: 0.8844 - val_categorical_accuracy: 0.6368\n",
      "Epoch 159/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8527 - categorical_accuracy: 0.6596 - val_loss: 0.8502 - val_categorical_accuracy: 0.6598\n",
      "Epoch 160/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8569 - categorical_accuracy: 0.6546 - val_loss: 0.8578 - val_categorical_accuracy: 0.6664\n",
      "Epoch 161/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8576 - categorical_accuracy: 0.6515 - val_loss: 0.8626 - val_categorical_accuracy: 0.6557\n",
      "Epoch 162/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8410 - categorical_accuracy: 0.6600 - val_loss: 0.8536 - val_categorical_accuracy: 0.6549\n",
      "Epoch 163/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8499 - categorical_accuracy: 0.6616 - val_loss: 0.8846 - val_categorical_accuracy: 0.6401\n",
      "Epoch 164/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8413 - categorical_accuracy: 0.6637 - val_loss: 0.8992 - val_categorical_accuracy: 0.6467\n",
      "Epoch 165/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8383 - categorical_accuracy: 0.6682 - val_loss: 0.8705 - val_categorical_accuracy: 0.6574\n",
      "Epoch 166/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8525 - categorical_accuracy: 0.6573 - val_loss: 0.8820 - val_categorical_accuracy: 0.6574\n",
      "Epoch 167/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8370 - categorical_accuracy: 0.6699 - val_loss: 0.8846 - val_categorical_accuracy: 0.6582\n",
      "Epoch 168/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8482 - categorical_accuracy: 0.6635 - val_loss: 0.8814 - val_categorical_accuracy: 0.6524\n",
      "Epoch 169/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8433 - categorical_accuracy: 0.6625 - val_loss: 0.8851 - val_categorical_accuracy: 0.6459\n",
      "Epoch 170/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8391 - categorical_accuracy: 0.6652 - val_loss: 0.8829 - val_categorical_accuracy: 0.6647\n",
      "Epoch 171/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8357 - categorical_accuracy: 0.6639 - val_loss: 0.8723 - val_categorical_accuracy: 0.6459\n",
      "Epoch 172/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8377 - categorical_accuracy: 0.6631 - val_loss: 0.8617 - val_categorical_accuracy: 0.6664\n",
      "Epoch 173/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8381 - categorical_accuracy: 0.6616 - val_loss: 0.8897 - val_categorical_accuracy: 0.6590\n",
      "Epoch 174/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8385 - categorical_accuracy: 0.6621 - val_loss: 0.8788 - val_categorical_accuracy: 0.6590\n",
      "Epoch 175/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8357 - categorical_accuracy: 0.6643 - val_loss: 0.8462 - val_categorical_accuracy: 0.6795\n",
      "Epoch 176/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8289 - categorical_accuracy: 0.6720 - val_loss: 0.8646 - val_categorical_accuracy: 0.6623\n",
      "Epoch 177/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8447 - categorical_accuracy: 0.6647 - val_loss: 0.8788 - val_categorical_accuracy: 0.6697\n",
      "Epoch 178/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8335 - categorical_accuracy: 0.6645 - val_loss: 0.8369 - val_categorical_accuracy: 0.6771\n",
      "Epoch 179/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8281 - categorical_accuracy: 0.6703 - val_loss: 0.8779 - val_categorical_accuracy: 0.6574\n",
      "Epoch 180/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8337 - categorical_accuracy: 0.6666 - val_loss: 0.8563 - val_categorical_accuracy: 0.6623\n",
      "Epoch 181/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8240 - categorical_accuracy: 0.6786 - val_loss: 0.8453 - val_categorical_accuracy: 0.6680\n",
      "Epoch 182/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8254 - categorical_accuracy: 0.6709 - val_loss: 0.9003 - val_categorical_accuracy: 0.6417\n",
      "Epoch 183/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8280 - categorical_accuracy: 0.6689 - val_loss: 0.8908 - val_categorical_accuracy: 0.6623\n",
      "Epoch 184/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8289 - categorical_accuracy: 0.6668 - val_loss: 0.9105 - val_categorical_accuracy: 0.6598\n",
      "Epoch 185/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8365 - categorical_accuracy: 0.6619 - val_loss: 0.8588 - val_categorical_accuracy: 0.6598\n",
      "Epoch 186/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8281 - categorical_accuracy: 0.6713 - val_loss: 0.8805 - val_categorical_accuracy: 0.6631\n",
      "Epoch 187/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8309 - categorical_accuracy: 0.6687 - val_loss: 0.8509 - val_categorical_accuracy: 0.6672\n",
      "Epoch 188/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8358 - categorical_accuracy: 0.6641 - val_loss: 0.9135 - val_categorical_accuracy: 0.6647\n",
      "Epoch 189/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8273 - categorical_accuracy: 0.6709 - val_loss: 0.9103 - val_categorical_accuracy: 0.6565\n",
      "Epoch 190/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8213 - categorical_accuracy: 0.6697 - val_loss: 0.8798 - val_categorical_accuracy: 0.6532\n",
      "Epoch 191/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8232 - categorical_accuracy: 0.6720 - val_loss: 0.8371 - val_categorical_accuracy: 0.6738\n",
      "Epoch 192/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8334 - categorical_accuracy: 0.6689 - val_loss: 0.8379 - val_categorical_accuracy: 0.6697\n",
      "Epoch 193/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8208 - categorical_accuracy: 0.6786 - val_loss: 0.8759 - val_categorical_accuracy: 0.6730\n",
      "Epoch 194/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8220 - categorical_accuracy: 0.6779 - val_loss: 0.8533 - val_categorical_accuracy: 0.6713\n",
      "Epoch 195/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8288 - categorical_accuracy: 0.6773 - val_loss: 0.8971 - val_categorical_accuracy: 0.6524\n",
      "Epoch 196/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8141 - categorical_accuracy: 0.6856 - val_loss: 0.8593 - val_categorical_accuracy: 0.6516\n",
      "Epoch 197/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8108 - categorical_accuracy: 0.6771 - val_loss: 0.8809 - val_categorical_accuracy: 0.6590\n",
      "Epoch 198/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8241 - categorical_accuracy: 0.6753 - val_loss: 0.8592 - val_categorical_accuracy: 0.6647\n",
      "Epoch 199/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8186 - categorical_accuracy: 0.6775 - val_loss: 0.8648 - val_categorical_accuracy: 0.6656\n",
      "Epoch 200/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8208 - categorical_accuracy: 0.6761 - val_loss: 0.8820 - val_categorical_accuracy: 0.6557\n",
      "Epoch 201/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8108 - categorical_accuracy: 0.6786 - val_loss: 0.8355 - val_categorical_accuracy: 0.6836\n",
      "Epoch 202/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8135 - categorical_accuracy: 0.6738 - val_loss: 0.8629 - val_categorical_accuracy: 0.6590\n",
      "Epoch 203/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8108 - categorical_accuracy: 0.6788 - val_loss: 0.8986 - val_categorical_accuracy: 0.6467\n",
      "Epoch 204/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8095 - categorical_accuracy: 0.6808 - val_loss: 0.8311 - val_categorical_accuracy: 0.6746\n",
      "Epoch 205/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8147 - categorical_accuracy: 0.6800 - val_loss: 0.8506 - val_categorical_accuracy: 0.6672\n",
      "Epoch 206/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8216 - categorical_accuracy: 0.6767 - val_loss: 0.8366 - val_categorical_accuracy: 0.6656\n",
      "Epoch 207/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8151 - categorical_accuracy: 0.6790 - val_loss: 0.8734 - val_categorical_accuracy: 0.6639\n",
      "Epoch 208/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8165 - categorical_accuracy: 0.6763 - val_loss: 0.8180 - val_categorical_accuracy: 0.6787\n",
      "Epoch 209/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8157 - categorical_accuracy: 0.6761 - val_loss: 0.9065 - val_categorical_accuracy: 0.6557\n",
      "Epoch 210/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8114 - categorical_accuracy: 0.6796 - val_loss: 0.8812 - val_categorical_accuracy: 0.6590\n",
      "Epoch 211/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8126 - categorical_accuracy: 0.6740 - val_loss: 0.9204 - val_categorical_accuracy: 0.6426\n",
      "Epoch 212/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8156 - categorical_accuracy: 0.6777 - val_loss: 0.9085 - val_categorical_accuracy: 0.6475\n",
      "Epoch 213/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8097 - categorical_accuracy: 0.6773 - val_loss: 0.8552 - val_categorical_accuracy: 0.6754\n",
      "Epoch 214/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8051 - categorical_accuracy: 0.6837 - val_loss: 0.8633 - val_categorical_accuracy: 0.6565\n",
      "Epoch 215/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8001 - categorical_accuracy: 0.6860 - val_loss: 0.8240 - val_categorical_accuracy: 0.6861\n",
      "Epoch 216/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8058 - categorical_accuracy: 0.6767 - val_loss: 0.8833 - val_categorical_accuracy: 0.6590\n",
      "Epoch 217/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8100 - categorical_accuracy: 0.6819 - val_loss: 0.8635 - val_categorical_accuracy: 0.6639\n",
      "Epoch 218/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7981 - categorical_accuracy: 0.6794 - val_loss: 0.8387 - val_categorical_accuracy: 0.6828\n",
      "Epoch 219/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7976 - categorical_accuracy: 0.6897 - val_loss: 0.8581 - val_categorical_accuracy: 0.6680\n",
      "Epoch 220/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8132 - categorical_accuracy: 0.6810 - val_loss: 0.8561 - val_categorical_accuracy: 0.6697\n",
      "Epoch 221/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8085 - categorical_accuracy: 0.6792 - val_loss: 0.8041 - val_categorical_accuracy: 0.6845\n",
      "Epoch 222/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8077 - categorical_accuracy: 0.6794 - val_loss: 0.8886 - val_categorical_accuracy: 0.6574\n",
      "Epoch 223/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7894 - categorical_accuracy: 0.6973 - val_loss: 0.8563 - val_categorical_accuracy: 0.6656\n",
      "Epoch 224/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8045 - categorical_accuracy: 0.6810 - val_loss: 0.8476 - val_categorical_accuracy: 0.6713\n",
      "Epoch 225/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7993 - categorical_accuracy: 0.6845 - val_loss: 0.8547 - val_categorical_accuracy: 0.6713\n",
      "Epoch 226/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8178 - categorical_accuracy: 0.6765 - val_loss: 0.8484 - val_categorical_accuracy: 0.6705\n",
      "Epoch 227/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.8027 - categorical_accuracy: 0.6794 - val_loss: 0.8307 - val_categorical_accuracy: 0.6763\n",
      "Epoch 228/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7960 - categorical_accuracy: 0.6868 - val_loss: 0.8533 - val_categorical_accuracy: 0.6763\n",
      "Epoch 229/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7992 - categorical_accuracy: 0.6883 - val_loss: 0.8468 - val_categorical_accuracy: 0.6779\n",
      "Epoch 230/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8106 - categorical_accuracy: 0.6753 - val_loss: 0.8534 - val_categorical_accuracy: 0.6697\n",
      "Epoch 231/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7910 - categorical_accuracy: 0.6932 - val_loss: 0.8500 - val_categorical_accuracy: 0.6779\n",
      "Epoch 232/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7951 - categorical_accuracy: 0.6843 - val_loss: 0.8498 - val_categorical_accuracy: 0.6672\n",
      "Epoch 233/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7937 - categorical_accuracy: 0.6833 - val_loss: 0.8434 - val_categorical_accuracy: 0.6730\n",
      "Epoch 234/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7913 - categorical_accuracy: 0.6895 - val_loss: 0.8359 - val_categorical_accuracy: 0.6705\n",
      "Epoch 235/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7969 - categorical_accuracy: 0.6872 - val_loss: 0.8477 - val_categorical_accuracy: 0.6705\n",
      "Epoch 236/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7881 - categorical_accuracy: 0.6899 - val_loss: 0.8406 - val_categorical_accuracy: 0.6730\n",
      "Epoch 237/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7909 - categorical_accuracy: 0.6835 - val_loss: 0.8377 - val_categorical_accuracy: 0.6721\n",
      "Epoch 238/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8000 - categorical_accuracy: 0.6802 - val_loss: 0.8394 - val_categorical_accuracy: 0.6730\n",
      "Epoch 239/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8029 - categorical_accuracy: 0.6850 - val_loss: 0.8626 - val_categorical_accuracy: 0.6623\n",
      "Epoch 240/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7915 - categorical_accuracy: 0.6862 - val_loss: 0.8953 - val_categorical_accuracy: 0.6582\n",
      "Epoch 241/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7933 - categorical_accuracy: 0.6891 - val_loss: 0.8717 - val_categorical_accuracy: 0.6680\n",
      "Epoch 242/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8016 - categorical_accuracy: 0.6800 - val_loss: 0.8093 - val_categorical_accuracy: 0.6771\n",
      "Epoch 243/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7920 - categorical_accuracy: 0.6858 - val_loss: 0.8405 - val_categorical_accuracy: 0.6697\n",
      "Epoch 244/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8034 - categorical_accuracy: 0.6815 - val_loss: 0.8318 - val_categorical_accuracy: 0.6787\n",
      "Epoch 245/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7963 - categorical_accuracy: 0.6823 - val_loss: 0.8492 - val_categorical_accuracy: 0.6705\n",
      "Epoch 246/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7937 - categorical_accuracy: 0.6866 - val_loss: 0.9230 - val_categorical_accuracy: 0.6450\n",
      "Epoch 247/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7865 - categorical_accuracy: 0.6907 - val_loss: 0.8584 - val_categorical_accuracy: 0.6680\n",
      "Epoch 248/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7899 - categorical_accuracy: 0.6856 - val_loss: 0.7949 - val_categorical_accuracy: 0.6853\n",
      "Epoch 249/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7881 - categorical_accuracy: 0.6852 - val_loss: 0.8133 - val_categorical_accuracy: 0.6771\n",
      "Epoch 250/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7905 - categorical_accuracy: 0.6905 - val_loss: 0.8310 - val_categorical_accuracy: 0.6738\n",
      "Epoch 251/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7859 - categorical_accuracy: 0.6924 - val_loss: 0.8068 - val_categorical_accuracy: 0.6968\n",
      "Epoch 252/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7945 - categorical_accuracy: 0.6914 - val_loss: 0.7894 - val_categorical_accuracy: 0.6910\n",
      "Epoch 253/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7904 - categorical_accuracy: 0.6903 - val_loss: 0.8199 - val_categorical_accuracy: 0.6754\n",
      "Epoch 254/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7913 - categorical_accuracy: 0.6934 - val_loss: 0.8319 - val_categorical_accuracy: 0.6746\n",
      "Epoch 255/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7832 - categorical_accuracy: 0.6907 - val_loss: 0.8170 - val_categorical_accuracy: 0.6878\n",
      "Epoch 256/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7875 - categorical_accuracy: 0.6953 - val_loss: 0.8857 - val_categorical_accuracy: 0.6623\n",
      "Epoch 257/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7989 - categorical_accuracy: 0.6885 - val_loss: 0.8538 - val_categorical_accuracy: 0.6664\n",
      "Epoch 258/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7908 - categorical_accuracy: 0.6864 - val_loss: 0.8911 - val_categorical_accuracy: 0.6557\n",
      "Epoch 259/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7828 - categorical_accuracy: 0.6988 - val_loss: 0.8781 - val_categorical_accuracy: 0.6606\n",
      "Epoch 260/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.8024 - categorical_accuracy: 0.6825 - val_loss: 0.8200 - val_categorical_accuracy: 0.6787\n",
      "Epoch 261/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7860 - categorical_accuracy: 0.6982 - val_loss: 0.8556 - val_categorical_accuracy: 0.6713\n",
      "Epoch 262/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7874 - categorical_accuracy: 0.6930 - val_loss: 0.8334 - val_categorical_accuracy: 0.6697\n",
      "Epoch 263/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7887 - categorical_accuracy: 0.6868 - val_loss: 0.8168 - val_categorical_accuracy: 0.6869\n",
      "Epoch 264/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7779 - categorical_accuracy: 0.6938 - val_loss: 0.7983 - val_categorical_accuracy: 0.6886\n",
      "Epoch 265/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7832 - categorical_accuracy: 0.6922 - val_loss: 0.8786 - val_categorical_accuracy: 0.6516\n",
      "Epoch 266/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7901 - categorical_accuracy: 0.6897 - val_loss: 0.8139 - val_categorical_accuracy: 0.6787\n",
      "Epoch 267/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7897 - categorical_accuracy: 0.6845 - val_loss: 0.8384 - val_categorical_accuracy: 0.6812\n",
      "Epoch 268/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7800 - categorical_accuracy: 0.6914 - val_loss: 0.8105 - val_categorical_accuracy: 0.6845\n",
      "Epoch 269/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7799 - categorical_accuracy: 0.6887 - val_loss: 0.8397 - val_categorical_accuracy: 0.6738\n",
      "Epoch 270/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7889 - categorical_accuracy: 0.6980 - val_loss: 0.8300 - val_categorical_accuracy: 0.6738\n",
      "Epoch 271/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7787 - categorical_accuracy: 0.6955 - val_loss: 0.8277 - val_categorical_accuracy: 0.6804\n",
      "Epoch 272/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7843 - categorical_accuracy: 0.6934 - val_loss: 0.8790 - val_categorical_accuracy: 0.6664\n",
      "Epoch 273/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7740 - categorical_accuracy: 0.7023 - val_loss: 0.7953 - val_categorical_accuracy: 0.6984\n",
      "Epoch 274/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7834 - categorical_accuracy: 0.6928 - val_loss: 0.8248 - val_categorical_accuracy: 0.6771\n",
      "Epoch 275/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7807 - categorical_accuracy: 0.6945 - val_loss: 0.8363 - val_categorical_accuracy: 0.6779\n",
      "Epoch 276/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7782 - categorical_accuracy: 0.7015 - val_loss: 0.8377 - val_categorical_accuracy: 0.6787\n",
      "Epoch 277/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7823 - categorical_accuracy: 0.6905 - val_loss: 0.8691 - val_categorical_accuracy: 0.6647\n",
      "Epoch 278/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7816 - categorical_accuracy: 0.6930 - val_loss: 0.8102 - val_categorical_accuracy: 0.6853\n",
      "Epoch 279/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7759 - categorical_accuracy: 0.6909 - val_loss: 0.8475 - val_categorical_accuracy: 0.6820\n",
      "Epoch 280/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7872 - categorical_accuracy: 0.6901 - val_loss: 0.8098 - val_categorical_accuracy: 0.6878\n",
      "Epoch 281/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7805 - categorical_accuracy: 0.6905 - val_loss: 0.8632 - val_categorical_accuracy: 0.6754\n",
      "Epoch 282/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7791 - categorical_accuracy: 0.6967 - val_loss: 0.8272 - val_categorical_accuracy: 0.6878\n",
      "Epoch 283/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7812 - categorical_accuracy: 0.6988 - val_loss: 0.8536 - val_categorical_accuracy: 0.6656\n",
      "Epoch 284/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7865 - categorical_accuracy: 0.6881 - val_loss: 0.8242 - val_categorical_accuracy: 0.6836\n",
      "Epoch 285/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7876 - categorical_accuracy: 0.6932 - val_loss: 0.8728 - val_categorical_accuracy: 0.6606\n",
      "Epoch 286/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7757 - categorical_accuracy: 0.6955 - val_loss: 0.8039 - val_categorical_accuracy: 0.6878\n",
      "Epoch 287/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7799 - categorical_accuracy: 0.6924 - val_loss: 0.8191 - val_categorical_accuracy: 0.6894\n",
      "Epoch 288/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7847 - categorical_accuracy: 0.6959 - val_loss: 0.8217 - val_categorical_accuracy: 0.6763\n",
      "Epoch 289/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7768 - categorical_accuracy: 0.6953 - val_loss: 0.8269 - val_categorical_accuracy: 0.6795\n",
      "Epoch 290/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7805 - categorical_accuracy: 0.6980 - val_loss: 0.8101 - val_categorical_accuracy: 0.6919\n",
      "Epoch 291/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7703 - categorical_accuracy: 0.7023 - val_loss: 0.8241 - val_categorical_accuracy: 0.6845\n",
      "Epoch 292/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7788 - categorical_accuracy: 0.6914 - val_loss: 0.8331 - val_categorical_accuracy: 0.6828\n",
      "Epoch 293/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7832 - categorical_accuracy: 0.6980 - val_loss: 0.8495 - val_categorical_accuracy: 0.6812\n",
      "Epoch 294/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7729 - categorical_accuracy: 0.6955 - val_loss: 0.8085 - val_categorical_accuracy: 0.6952\n",
      "Epoch 295/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7767 - categorical_accuracy: 0.6988 - val_loss: 0.8480 - val_categorical_accuracy: 0.6705\n",
      "Epoch 296/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7869 - categorical_accuracy: 0.6971 - val_loss: 0.7935 - val_categorical_accuracy: 0.6910\n",
      "Epoch 297/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7803 - categorical_accuracy: 0.6909 - val_loss: 0.8088 - val_categorical_accuracy: 0.6878\n",
      "Epoch 298/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7739 - categorical_accuracy: 0.7006 - val_loss: 0.8126 - val_categorical_accuracy: 0.6861\n",
      "Epoch 299/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7699 - categorical_accuracy: 0.7013 - val_loss: 0.8652 - val_categorical_accuracy: 0.6631\n",
      "Epoch 300/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7653 - categorical_accuracy: 0.7039 - val_loss: 0.8204 - val_categorical_accuracy: 0.6812\n",
      "Epoch 301/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7676 - categorical_accuracy: 0.7050 - val_loss: 0.8763 - val_categorical_accuracy: 0.6746\n",
      "Epoch 302/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7659 - categorical_accuracy: 0.6961 - val_loss: 0.8474 - val_categorical_accuracy: 0.6721\n",
      "Epoch 303/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7766 - categorical_accuracy: 0.6938 - val_loss: 0.8322 - val_categorical_accuracy: 0.6721\n",
      "Epoch 304/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7754 - categorical_accuracy: 0.6942 - val_loss: 0.8531 - val_categorical_accuracy: 0.6656\n",
      "Epoch 305/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7792 - categorical_accuracy: 0.6978 - val_loss: 0.8426 - val_categorical_accuracy: 0.6746\n",
      "Epoch 306/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7573 - categorical_accuracy: 0.7035 - val_loss: 0.8537 - val_categorical_accuracy: 0.6713\n",
      "Epoch 307/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7655 - categorical_accuracy: 0.6961 - val_loss: 0.8082 - val_categorical_accuracy: 0.6894\n",
      "Epoch 308/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7744 - categorical_accuracy: 0.6942 - val_loss: 0.8624 - val_categorical_accuracy: 0.6763\n",
      "Epoch 309/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7644 - categorical_accuracy: 0.7037 - val_loss: 0.8281 - val_categorical_accuracy: 0.6853\n",
      "Epoch 310/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7770 - categorical_accuracy: 0.7013 - val_loss: 0.8272 - val_categorical_accuracy: 0.6787\n",
      "Epoch 311/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7647 - categorical_accuracy: 0.7019 - val_loss: 0.8534 - val_categorical_accuracy: 0.6763\n",
      "Epoch 312/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7648 - categorical_accuracy: 0.6986 - val_loss: 0.8418 - val_categorical_accuracy: 0.6763\n",
      "Epoch 313/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7625 - categorical_accuracy: 0.7035 - val_loss: 0.8265 - val_categorical_accuracy: 0.6804\n",
      "Epoch 314/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7661 - categorical_accuracy: 0.7002 - val_loss: 0.8536 - val_categorical_accuracy: 0.6738\n",
      "Epoch 315/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7631 - categorical_accuracy: 0.7029 - val_loss: 0.8007 - val_categorical_accuracy: 0.6919\n",
      "Epoch 316/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7643 - categorical_accuracy: 0.7097 - val_loss: 0.8361 - val_categorical_accuracy: 0.6820\n",
      "Epoch 317/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7622 - categorical_accuracy: 0.6967 - val_loss: 0.7935 - val_categorical_accuracy: 0.6894\n",
      "Epoch 318/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7709 - categorical_accuracy: 0.7039 - val_loss: 0.8392 - val_categorical_accuracy: 0.6795\n",
      "Epoch 319/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7586 - categorical_accuracy: 0.7068 - val_loss: 0.8296 - val_categorical_accuracy: 0.6910\n",
      "Epoch 320/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7614 - categorical_accuracy: 0.7000 - val_loss: 0.8557 - val_categorical_accuracy: 0.6795\n",
      "Epoch 321/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7699 - categorical_accuracy: 0.6994 - val_loss: 0.8203 - val_categorical_accuracy: 0.6861\n",
      "Epoch 322/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7669 - categorical_accuracy: 0.6990 - val_loss: 0.8389 - val_categorical_accuracy: 0.6779\n",
      "Epoch 323/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7587 - categorical_accuracy: 0.7058 - val_loss: 0.8631 - val_categorical_accuracy: 0.6672\n",
      "Epoch 324/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7654 - categorical_accuracy: 0.6984 - val_loss: 0.8553 - val_categorical_accuracy: 0.6680\n",
      "Epoch 325/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7745 - categorical_accuracy: 0.6947 - val_loss: 0.8364 - val_categorical_accuracy: 0.6746\n",
      "Epoch 326/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7732 - categorical_accuracy: 0.7008 - val_loss: 0.8279 - val_categorical_accuracy: 0.6746\n",
      "Epoch 327/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7616 - categorical_accuracy: 0.7023 - val_loss: 0.8039 - val_categorical_accuracy: 0.6952\n",
      "Epoch 328/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7631 - categorical_accuracy: 0.7041 - val_loss: 0.8140 - val_categorical_accuracy: 0.6836\n",
      "Epoch 329/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7648 - categorical_accuracy: 0.7019 - val_loss: 0.8342 - val_categorical_accuracy: 0.6771\n",
      "Epoch 330/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7621 - categorical_accuracy: 0.6980 - val_loss: 0.8185 - val_categorical_accuracy: 0.6861\n",
      "Epoch 331/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7560 - categorical_accuracy: 0.7081 - val_loss: 0.8254 - val_categorical_accuracy: 0.6795\n",
      "Epoch 332/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7603 - categorical_accuracy: 0.7103 - val_loss: 0.8348 - val_categorical_accuracy: 0.6828\n",
      "Epoch 333/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7727 - categorical_accuracy: 0.6945 - val_loss: 0.8288 - val_categorical_accuracy: 0.6836\n",
      "Epoch 334/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7716 - categorical_accuracy: 0.7002 - val_loss: 0.8255 - val_categorical_accuracy: 0.6836\n",
      "Epoch 335/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7688 - categorical_accuracy: 0.7013 - val_loss: 0.8144 - val_categorical_accuracy: 0.6836\n",
      "Epoch 336/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7731 - categorical_accuracy: 0.6990 - val_loss: 0.8623 - val_categorical_accuracy: 0.6804\n",
      "Epoch 337/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7738 - categorical_accuracy: 0.6982 - val_loss: 0.8029 - val_categorical_accuracy: 0.6902\n",
      "Epoch 338/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7573 - categorical_accuracy: 0.7087 - val_loss: 0.8150 - val_categorical_accuracy: 0.6779\n",
      "Epoch 339/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7592 - categorical_accuracy: 0.7062 - val_loss: 0.8301 - val_categorical_accuracy: 0.6886\n",
      "Epoch 340/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7600 - categorical_accuracy: 0.7037 - val_loss: 0.8604 - val_categorical_accuracy: 0.6721\n",
      "Epoch 341/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7667 - categorical_accuracy: 0.7029 - val_loss: 0.8625 - val_categorical_accuracy: 0.6771\n",
      "Epoch 342/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7470 - categorical_accuracy: 0.7060 - val_loss: 0.8374 - val_categorical_accuracy: 0.6828\n",
      "Epoch 343/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7566 - categorical_accuracy: 0.7052 - val_loss: 0.8441 - val_categorical_accuracy: 0.6787\n",
      "Epoch 344/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7517 - categorical_accuracy: 0.7085 - val_loss: 0.8314 - val_categorical_accuracy: 0.6845\n",
      "Epoch 345/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7522 - categorical_accuracy: 0.7116 - val_loss: 0.8270 - val_categorical_accuracy: 0.6820\n",
      "Epoch 346/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7748 - categorical_accuracy: 0.6893 - val_loss: 0.7888 - val_categorical_accuracy: 0.7025\n",
      "Epoch 347/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7649 - categorical_accuracy: 0.7062 - val_loss: 0.8342 - val_categorical_accuracy: 0.6779\n",
      "Epoch 348/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7574 - categorical_accuracy: 0.7081 - val_loss: 0.8378 - val_categorical_accuracy: 0.6836\n",
      "Epoch 349/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7610 - categorical_accuracy: 0.7037 - val_loss: 0.8413 - val_categorical_accuracy: 0.6836\n",
      "Epoch 350/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7571 - categorical_accuracy: 0.7068 - val_loss: 0.8410 - val_categorical_accuracy: 0.6828\n",
      "Epoch 351/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7563 - categorical_accuracy: 0.7064 - val_loss: 0.8270 - val_categorical_accuracy: 0.6861\n",
      "Epoch 352/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7619 - categorical_accuracy: 0.6998 - val_loss: 0.8330 - val_categorical_accuracy: 0.6836\n",
      "Epoch 353/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7720 - categorical_accuracy: 0.6978 - val_loss: 0.8350 - val_categorical_accuracy: 0.6787\n",
      "Epoch 354/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7593 - categorical_accuracy: 0.7066 - val_loss: 0.8210 - val_categorical_accuracy: 0.6845\n",
      "Epoch 355/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7543 - categorical_accuracy: 0.7116 - val_loss: 0.8012 - val_categorical_accuracy: 0.6943\n",
      "Epoch 356/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7443 - categorical_accuracy: 0.7184 - val_loss: 0.8452 - val_categorical_accuracy: 0.6779\n",
      "Epoch 357/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7507 - categorical_accuracy: 0.7136 - val_loss: 0.8093 - val_categorical_accuracy: 0.7025\n",
      "Epoch 358/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7527 - categorical_accuracy: 0.7060 - val_loss: 0.8125 - val_categorical_accuracy: 0.6845\n",
      "Epoch 359/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7631 - categorical_accuracy: 0.7006 - val_loss: 0.8041 - val_categorical_accuracy: 0.6902\n",
      "Epoch 360/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7398 - categorical_accuracy: 0.7130 - val_loss: 0.7931 - val_categorical_accuracy: 0.6919\n",
      "Epoch 361/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7592 - categorical_accuracy: 0.7023 - val_loss: 0.8143 - val_categorical_accuracy: 0.6943\n",
      "Epoch 362/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7618 - categorical_accuracy: 0.7004 - val_loss: 0.8394 - val_categorical_accuracy: 0.6771\n",
      "Epoch 363/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7618 - categorical_accuracy: 0.7041 - val_loss: 0.8305 - val_categorical_accuracy: 0.6779\n",
      "Epoch 364/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7538 - categorical_accuracy: 0.7031 - val_loss: 0.8232 - val_categorical_accuracy: 0.6886\n",
      "Epoch 365/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7450 - categorical_accuracy: 0.7149 - val_loss: 0.7958 - val_categorical_accuracy: 0.6927\n",
      "Epoch 366/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7552 - categorical_accuracy: 0.7046 - val_loss: 0.8268 - val_categorical_accuracy: 0.6730\n",
      "Epoch 367/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7542 - categorical_accuracy: 0.7074 - val_loss: 0.8840 - val_categorical_accuracy: 0.6680\n",
      "Epoch 368/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7525 - categorical_accuracy: 0.7056 - val_loss: 0.8249 - val_categorical_accuracy: 0.6845\n",
      "Epoch 369/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7584 - categorical_accuracy: 0.7052 - val_loss: 0.8224 - val_categorical_accuracy: 0.6910\n",
      "Epoch 370/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7490 - categorical_accuracy: 0.7066 - val_loss: 0.8283 - val_categorical_accuracy: 0.6779\n",
      "Epoch 371/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7543 - categorical_accuracy: 0.7093 - val_loss: 0.8519 - val_categorical_accuracy: 0.6812\n",
      "Epoch 372/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7473 - categorical_accuracy: 0.7103 - val_loss: 0.8036 - val_categorical_accuracy: 0.6960\n",
      "Epoch 373/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7558 - categorical_accuracy: 0.7002 - val_loss: 0.8015 - val_categorical_accuracy: 0.6910\n",
      "Epoch 374/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7569 - categorical_accuracy: 0.7046 - val_loss: 0.8143 - val_categorical_accuracy: 0.6910\n",
      "Epoch 375/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7509 - categorical_accuracy: 0.7130 - val_loss: 0.8637 - val_categorical_accuracy: 0.6779\n",
      "Epoch 376/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7571 - categorical_accuracy: 0.7114 - val_loss: 0.7819 - val_categorical_accuracy: 0.7001\n",
      "Epoch 377/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7535 - categorical_accuracy: 0.7070 - val_loss: 0.8233 - val_categorical_accuracy: 0.6836\n",
      "Epoch 378/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7528 - categorical_accuracy: 0.7070 - val_loss: 0.7904 - val_categorical_accuracy: 0.7001\n",
      "Epoch 379/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7576 - categorical_accuracy: 0.7004 - val_loss: 0.8302 - val_categorical_accuracy: 0.6861\n",
      "Epoch 380/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7458 - categorical_accuracy: 0.7110 - val_loss: 0.8778 - val_categorical_accuracy: 0.6689\n",
      "Epoch 381/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7511 - categorical_accuracy: 0.7079 - val_loss: 0.8621 - val_categorical_accuracy: 0.6836\n",
      "Epoch 382/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7602 - categorical_accuracy: 0.6994 - val_loss: 0.8080 - val_categorical_accuracy: 0.6943\n",
      "Epoch 383/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7469 - categorical_accuracy: 0.7114 - val_loss: 0.8332 - val_categorical_accuracy: 0.6812\n",
      "Epoch 384/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7541 - categorical_accuracy: 0.7035 - val_loss: 0.7894 - val_categorical_accuracy: 0.6968\n",
      "Epoch 385/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7496 - categorical_accuracy: 0.7089 - val_loss: 0.8264 - val_categorical_accuracy: 0.6861\n",
      "Epoch 386/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7541 - categorical_accuracy: 0.7060 - val_loss: 0.8794 - val_categorical_accuracy: 0.6746\n",
      "Epoch 387/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7563 - categorical_accuracy: 0.7128 - val_loss: 0.7910 - val_categorical_accuracy: 0.7042\n",
      "Epoch 388/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7555 - categorical_accuracy: 0.7126 - val_loss: 0.8430 - val_categorical_accuracy: 0.6795\n",
      "Epoch 389/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7534 - categorical_accuracy: 0.7140 - val_loss: 0.8030 - val_categorical_accuracy: 0.6952\n",
      "Epoch 390/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7483 - categorical_accuracy: 0.7112 - val_loss: 0.8111 - val_categorical_accuracy: 0.6919\n",
      "Epoch 391/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7569 - categorical_accuracy: 0.7002 - val_loss: 0.8119 - val_categorical_accuracy: 0.6919\n",
      "Epoch 392/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7522 - categorical_accuracy: 0.7097 - val_loss: 0.8481 - val_categorical_accuracy: 0.6836\n",
      "Epoch 393/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7497 - categorical_accuracy: 0.7107 - val_loss: 0.8045 - val_categorical_accuracy: 0.6836\n",
      "Epoch 394/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7436 - categorical_accuracy: 0.7116 - val_loss: 0.8325 - val_categorical_accuracy: 0.6869\n",
      "Epoch 395/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7411 - categorical_accuracy: 0.7118 - val_loss: 0.8090 - val_categorical_accuracy: 0.6960\n",
      "Epoch 396/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7405 - categorical_accuracy: 0.7105 - val_loss: 0.8377 - val_categorical_accuracy: 0.6828\n",
      "Epoch 397/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7487 - categorical_accuracy: 0.7103 - val_loss: 0.7894 - val_categorical_accuracy: 0.7017\n",
      "Epoch 398/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7421 - categorical_accuracy: 0.7077 - val_loss: 0.7978 - val_categorical_accuracy: 0.6927\n",
      "Epoch 399/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7604 - categorical_accuracy: 0.7066 - val_loss: 0.7954 - val_categorical_accuracy: 0.7017\n",
      "Epoch 400/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7507 - categorical_accuracy: 0.7099 - val_loss: 0.8280 - val_categorical_accuracy: 0.6869\n",
      "Epoch 401/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7642 - categorical_accuracy: 0.6992 - val_loss: 0.8052 - val_categorical_accuracy: 0.6886\n",
      "Epoch 402/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7486 - categorical_accuracy: 0.7174 - val_loss: 0.8259 - val_categorical_accuracy: 0.6869\n",
      "Epoch 403/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7480 - categorical_accuracy: 0.7157 - val_loss: 0.8310 - val_categorical_accuracy: 0.6795\n",
      "Epoch 404/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7507 - categorical_accuracy: 0.7091 - val_loss: 0.8147 - val_categorical_accuracy: 0.6993\n",
      "Epoch 405/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7605 - categorical_accuracy: 0.7070 - val_loss: 0.8361 - val_categorical_accuracy: 0.6853\n",
      "Epoch 406/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7565 - categorical_accuracy: 0.6986 - val_loss: 0.8210 - val_categorical_accuracy: 0.6927\n",
      "Epoch 407/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7446 - categorical_accuracy: 0.7126 - val_loss: 0.8052 - val_categorical_accuracy: 0.6943\n",
      "Epoch 408/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7456 - categorical_accuracy: 0.7140 - val_loss: 0.8276 - val_categorical_accuracy: 0.6853\n",
      "Epoch 409/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7414 - categorical_accuracy: 0.7165 - val_loss: 0.8235 - val_categorical_accuracy: 0.6886\n",
      "Epoch 410/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7474 - categorical_accuracy: 0.7095 - val_loss: 0.8361 - val_categorical_accuracy: 0.6787\n",
      "Epoch 411/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7435 - categorical_accuracy: 0.7099 - val_loss: 0.8043 - val_categorical_accuracy: 0.6943\n",
      "Epoch 412/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7440 - categorical_accuracy: 0.7132 - val_loss: 0.8224 - val_categorical_accuracy: 0.6910\n",
      "Epoch 413/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7409 - categorical_accuracy: 0.7136 - val_loss: 0.8075 - val_categorical_accuracy: 0.6952\n",
      "Epoch 414/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7456 - categorical_accuracy: 0.7074 - val_loss: 0.8036 - val_categorical_accuracy: 0.7017\n",
      "Epoch 415/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7446 - categorical_accuracy: 0.7101 - val_loss: 0.7821 - val_categorical_accuracy: 0.6976\n",
      "Epoch 416/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7435 - categorical_accuracy: 0.7128 - val_loss: 0.8465 - val_categorical_accuracy: 0.6812\n",
      "Epoch 417/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7442 - categorical_accuracy: 0.7112 - val_loss: 0.8106 - val_categorical_accuracy: 0.7034\n",
      "Epoch 418/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7387 - categorical_accuracy: 0.7159 - val_loss: 0.8050 - val_categorical_accuracy: 0.6910\n",
      "Epoch 419/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7387 - categorical_accuracy: 0.7124 - val_loss: 0.8527 - val_categorical_accuracy: 0.6910\n",
      "Epoch 420/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7353 - categorical_accuracy: 0.7211 - val_loss: 0.8723 - val_categorical_accuracy: 0.6861\n",
      "Epoch 421/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7416 - categorical_accuracy: 0.7116 - val_loss: 0.8215 - val_categorical_accuracy: 0.6919\n",
      "Epoch 422/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7527 - categorical_accuracy: 0.7029 - val_loss: 0.8593 - val_categorical_accuracy: 0.6861\n",
      "Epoch 423/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7447 - categorical_accuracy: 0.7153 - val_loss: 0.8085 - val_categorical_accuracy: 0.6952\n",
      "Epoch 424/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7518 - categorical_accuracy: 0.7112 - val_loss: 0.8350 - val_categorical_accuracy: 0.6869\n",
      "Epoch 425/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7574 - categorical_accuracy: 0.7056 - val_loss: 0.7852 - val_categorical_accuracy: 0.7083\n",
      "Epoch 426/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7368 - categorical_accuracy: 0.7244 - val_loss: 0.7987 - val_categorical_accuracy: 0.7017\n",
      "Epoch 427/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7346 - categorical_accuracy: 0.7171 - val_loss: 0.8096 - val_categorical_accuracy: 0.7009\n",
      "Epoch 428/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7547 - categorical_accuracy: 0.7002 - val_loss: 0.8156 - val_categorical_accuracy: 0.6935\n",
      "Epoch 429/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7484 - categorical_accuracy: 0.7116 - val_loss: 0.8077 - val_categorical_accuracy: 0.6894\n",
      "Epoch 430/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7372 - categorical_accuracy: 0.7105 - val_loss: 0.7980 - val_categorical_accuracy: 0.7034\n",
      "Epoch 431/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7509 - categorical_accuracy: 0.7089 - val_loss: 0.8461 - val_categorical_accuracy: 0.6861\n",
      "Epoch 432/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7395 - categorical_accuracy: 0.7081 - val_loss: 0.8060 - val_categorical_accuracy: 0.6943\n",
      "Epoch 433/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7486 - categorical_accuracy: 0.7074 - val_loss: 0.8306 - val_categorical_accuracy: 0.6869\n",
      "Epoch 434/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7380 - categorical_accuracy: 0.7116 - val_loss: 0.8145 - val_categorical_accuracy: 0.6968\n",
      "Epoch 435/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7493 - categorical_accuracy: 0.7097 - val_loss: 0.8707 - val_categorical_accuracy: 0.6763\n",
      "Epoch 436/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7511 - categorical_accuracy: 0.7081 - val_loss: 0.8111 - val_categorical_accuracy: 0.6869\n",
      "Epoch 437/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7354 - categorical_accuracy: 0.7198 - val_loss: 0.8171 - val_categorical_accuracy: 0.7034\n",
      "Epoch 438/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7385 - categorical_accuracy: 0.7151 - val_loss: 0.8150 - val_categorical_accuracy: 0.6968\n",
      "Epoch 439/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7389 - categorical_accuracy: 0.7211 - val_loss: 0.8106 - val_categorical_accuracy: 0.6943\n",
      "Epoch 440/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7382 - categorical_accuracy: 0.7151 - val_loss: 0.8266 - val_categorical_accuracy: 0.6952\n",
      "Epoch 441/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7406 - categorical_accuracy: 0.7180 - val_loss: 0.8612 - val_categorical_accuracy: 0.6845\n",
      "Epoch 442/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7396 - categorical_accuracy: 0.7155 - val_loss: 0.8385 - val_categorical_accuracy: 0.6886\n",
      "Epoch 443/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7395 - categorical_accuracy: 0.7143 - val_loss: 0.8093 - val_categorical_accuracy: 0.6976\n",
      "Epoch 444/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7464 - categorical_accuracy: 0.7041 - val_loss: 0.8526 - val_categorical_accuracy: 0.6869\n",
      "Epoch 445/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7265 - categorical_accuracy: 0.7174 - val_loss: 0.8115 - val_categorical_accuracy: 0.6960\n",
      "Epoch 446/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7371 - categorical_accuracy: 0.7120 - val_loss: 0.7863 - val_categorical_accuracy: 0.7017\n",
      "Epoch 447/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7474 - categorical_accuracy: 0.7089 - val_loss: 0.8209 - val_categorical_accuracy: 0.6943\n",
      "Epoch 448/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7510 - categorical_accuracy: 0.7046 - val_loss: 0.8460 - val_categorical_accuracy: 0.6878\n",
      "Epoch 449/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7350 - categorical_accuracy: 0.7182 - val_loss: 0.7974 - val_categorical_accuracy: 0.7001\n",
      "Epoch 450/500\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.7295 - categorical_accuracy: 0.7120 - val_loss: 0.7995 - val_categorical_accuracy: 0.7099\n",
      "Epoch 451/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7527 - categorical_accuracy: 0.7095 - val_loss: 0.8104 - val_categorical_accuracy: 0.6902\n",
      "Epoch 452/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7444 - categorical_accuracy: 0.7167 - val_loss: 0.9031 - val_categorical_accuracy: 0.6713\n",
      "Epoch 453/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7381 - categorical_accuracy: 0.7101 - val_loss: 0.8235 - val_categorical_accuracy: 0.6976\n",
      "Epoch 454/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7373 - categorical_accuracy: 0.7134 - val_loss: 0.8244 - val_categorical_accuracy: 0.6952\n",
      "Epoch 455/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7435 - categorical_accuracy: 0.7124 - val_loss: 0.8323 - val_categorical_accuracy: 0.6919\n",
      "Epoch 456/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7349 - categorical_accuracy: 0.7130 - val_loss: 0.8148 - val_categorical_accuracy: 0.6886\n",
      "Epoch 457/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7456 - categorical_accuracy: 0.7110 - val_loss: 0.8616 - val_categorical_accuracy: 0.6820\n",
      "Epoch 458/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7349 - categorical_accuracy: 0.7155 - val_loss: 0.8537 - val_categorical_accuracy: 0.6845\n",
      "Epoch 459/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7490 - categorical_accuracy: 0.7091 - val_loss: 0.8110 - val_categorical_accuracy: 0.6902\n",
      "Epoch 460/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7415 - categorical_accuracy: 0.7153 - val_loss: 0.8451 - val_categorical_accuracy: 0.6853\n",
      "Epoch 461/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7353 - categorical_accuracy: 0.7221 - val_loss: 0.8052 - val_categorical_accuracy: 0.6993\n",
      "Epoch 462/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7455 - categorical_accuracy: 0.7122 - val_loss: 0.8655 - val_categorical_accuracy: 0.6746\n",
      "Epoch 463/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7342 - categorical_accuracy: 0.7118 - val_loss: 0.8143 - val_categorical_accuracy: 0.6952\n",
      "Epoch 464/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7393 - categorical_accuracy: 0.7124 - val_loss: 0.8362 - val_categorical_accuracy: 0.6902\n",
      "Epoch 465/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7341 - categorical_accuracy: 0.7169 - val_loss: 0.8146 - val_categorical_accuracy: 0.6952\n",
      "Epoch 466/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7460 - categorical_accuracy: 0.7105 - val_loss: 0.8263 - val_categorical_accuracy: 0.6878\n",
      "Epoch 467/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7237 - categorical_accuracy: 0.7202 - val_loss: 0.8196 - val_categorical_accuracy: 0.6976\n",
      "Epoch 468/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7418 - categorical_accuracy: 0.7149 - val_loss: 0.8113 - val_categorical_accuracy: 0.6943\n",
      "Epoch 469/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7369 - categorical_accuracy: 0.7169 - val_loss: 0.8218 - val_categorical_accuracy: 0.6960\n",
      "Epoch 470/500\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.7336 - categorical_accuracy: 0.7215 - val_loss: 0.8216 - val_categorical_accuracy: 0.6927\n",
      "Epoch 471/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7369 - categorical_accuracy: 0.7147 - val_loss: 0.8041 - val_categorical_accuracy: 0.6976\n",
      "Epoch 472/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7419 - categorical_accuracy: 0.7105 - val_loss: 0.7831 - val_categorical_accuracy: 0.7108\n",
      "Epoch 473/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7242 - categorical_accuracy: 0.7207 - val_loss: 0.7931 - val_categorical_accuracy: 0.7067\n",
      "Epoch 474/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7398 - categorical_accuracy: 0.7174 - val_loss: 0.8156 - val_categorical_accuracy: 0.6927\n",
      "Epoch 475/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7400 - categorical_accuracy: 0.7163 - val_loss: 0.7998 - val_categorical_accuracy: 0.7001\n",
      "Epoch 476/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7376 - categorical_accuracy: 0.7217 - val_loss: 0.7947 - val_categorical_accuracy: 0.7050\n",
      "Epoch 477/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7329 - categorical_accuracy: 0.7244 - val_loss: 0.8364 - val_categorical_accuracy: 0.6902\n",
      "Epoch 478/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7368 - categorical_accuracy: 0.7145 - val_loss: 0.8196 - val_categorical_accuracy: 0.6927\n",
      "Epoch 479/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7349 - categorical_accuracy: 0.7198 - val_loss: 0.7915 - val_categorical_accuracy: 0.7083\n",
      "Epoch 480/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7367 - categorical_accuracy: 0.7097 - val_loss: 0.8693 - val_categorical_accuracy: 0.6853\n",
      "Epoch 481/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7247 - categorical_accuracy: 0.7285 - val_loss: 0.8255 - val_categorical_accuracy: 0.6976\n",
      "Epoch 482/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7362 - categorical_accuracy: 0.7194 - val_loss: 0.8338 - val_categorical_accuracy: 0.6927\n",
      "Epoch 483/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7388 - categorical_accuracy: 0.7221 - val_loss: 0.8227 - val_categorical_accuracy: 0.6927\n",
      "Epoch 484/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7283 - categorical_accuracy: 0.7204 - val_loss: 0.8454 - val_categorical_accuracy: 0.6886\n",
      "Epoch 485/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7372 - categorical_accuracy: 0.7235 - val_loss: 0.8087 - val_categorical_accuracy: 0.6984\n",
      "Epoch 486/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7313 - categorical_accuracy: 0.7161 - val_loss: 0.8273 - val_categorical_accuracy: 0.6984\n",
      "Epoch 487/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7317 - categorical_accuracy: 0.7221 - val_loss: 0.8089 - val_categorical_accuracy: 0.6993\n",
      "Epoch 488/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7310 - categorical_accuracy: 0.7244 - val_loss: 0.8187 - val_categorical_accuracy: 0.6993\n",
      "Epoch 489/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7329 - categorical_accuracy: 0.7198 - val_loss: 0.8080 - val_categorical_accuracy: 0.6960\n",
      "Epoch 490/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7375 - categorical_accuracy: 0.7165 - val_loss: 0.8172 - val_categorical_accuracy: 0.6927\n",
      "Epoch 491/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7250 - categorical_accuracy: 0.7178 - val_loss: 0.8047 - val_categorical_accuracy: 0.7067\n",
      "Epoch 492/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7440 - categorical_accuracy: 0.7072 - val_loss: 0.8571 - val_categorical_accuracy: 0.6894\n",
      "Epoch 493/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7280 - categorical_accuracy: 0.7192 - val_loss: 0.7961 - val_categorical_accuracy: 0.7083\n",
      "Epoch 494/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7282 - categorical_accuracy: 0.7171 - val_loss: 0.8225 - val_categorical_accuracy: 0.6960\n",
      "Epoch 495/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7304 - categorical_accuracy: 0.7223 - val_loss: 0.8380 - val_categorical_accuracy: 0.6960\n",
      "Epoch 496/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7340 - categorical_accuracy: 0.7169 - val_loss: 0.8296 - val_categorical_accuracy: 0.6910\n",
      "Epoch 497/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7399 - categorical_accuracy: 0.7116 - val_loss: 0.8141 - val_categorical_accuracy: 0.6943\n",
      "Epoch 498/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7243 - categorical_accuracy: 0.7250 - val_loss: 0.8285 - val_categorical_accuracy: 0.6910\n",
      "Epoch 499/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7345 - categorical_accuracy: 0.7165 - val_loss: 0.8202 - val_categorical_accuracy: 0.6993\n",
      "Epoch 500/500\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.7311 - categorical_accuracy: 0.7165 - val_loss: 0.8313 - val_categorical_accuracy: 0.6869\n"
     ]
    }
   ],
   "source": [
    "model = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1), convDropRate=0.4, encDropRate=0.4)\n",
    "model.summary()\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=500,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=200,\n",
    "                    validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "loss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          1.413193702697754,
          1.1484320163726807,
          1.1240897178649902,
          1.1147232055664062,
          1.0926225185394287,
          1.0855658054351807,
          1.0875153541564941,
          1.0778529644012451,
          1.0632903575897217,
          1.0635063648223877,
          1.0675033330917358,
          1.0592097043991089,
          1.0555551052093506,
          1.0513076782226562,
          1.0545111894607544,
          1.0502874851226807,
          1.0412054061889648,
          1.0433622598648071,
          1.0430822372436523,
          1.042554497718811,
          1.0273369550704956,
          1.0367330312728882,
          1.0295826196670532,
          1.0246502161026,
          1.0252668857574463,
          1.0252916812896729,
          1.0202293395996094,
          1.0217865705490112,
          1.0115282535552979,
          1.0108723640441895,
          1.010190486907959,
          1.0228019952774048,
          1.018391728401184,
          1.0089977979660034,
          1.0070849657058716,
          1.0061256885528564,
          1.0008697509765625,
          1.0001784563064575,
          1.0084261894226074,
          0.995701253414154,
          1.0069500207901,
          0.9932833313941956,
          0.9931502342224121,
          0.9996997714042664,
          0.9870527386665344,
          0.9880440831184387,
          0.9870642423629761,
          0.9813460111618042,
          0.9880741238594055,
          0.9816620945930481,
          0.9806716442108154,
          0.9837790131568909,
          0.9733630418777466,
          0.9838669896125793,
          0.9784621596336365,
          0.9688818454742432,
          0.9664096236228943,
          0.9695836305618286,
          0.9720770120620728,
          0.9640594124794006,
          0.9639630913734436,
          0.9580882787704468,
          0.9565246105194092,
          0.9603741765022278,
          0.9589908123016357,
          0.9563349485397339,
          0.9564187526702881,
          0.9477145075798035,
          0.9510355591773987,
          0.95111083984375,
          0.9555447697639465,
          0.9480116963386536,
          0.9567327499389648,
          0.9405458569526672,
          0.9495193362236023,
          0.9404869079589844,
          0.9522351622581482,
          0.941665530204773,
          0.9386143684387207,
          0.9361516237258911,
          0.9334948062896729,
          0.9304438233375549,
          0.9349703192710876,
          0.9354400634765625,
          0.9260000586509705,
          0.9256570339202881,
          0.9170891046524048,
          0.9284988641738892,
          0.9192909598350525,
          0.9189743399620056,
          0.9107497930526733,
          0.9217700362205505,
          0.9234898686408997,
          0.9177421927452087,
          0.912712812423706,
          0.921004593372345,
          0.9183731079101562,
          0.906199038028717,
          0.9139417409896851,
          0.9125185608863831,
          0.9118657112121582,
          0.920238196849823,
          0.9048789143562317,
          0.9169949889183044,
          0.9122151732444763,
          0.9039640426635742,
          0.9039069414138794,
          0.8896145820617676,
          0.8896510601043701,
          0.9031524062156677,
          0.8920254707336426,
          0.8984030485153198,
          0.8908514380455017,
          0.8957763910293579,
          0.8850867748260498,
          0.8935297131538391,
          0.8979238271713257,
          0.8902528882026672,
          0.8831745386123657,
          0.8974520564079285,
          0.8936086893081665,
          0.8764845728874207,
          0.8918183445930481,
          0.88059002161026,
          0.8827895522117615,
          0.8778353929519653,
          0.8861027359962463,
          0.8769363760948181,
          0.873423159122467,
          0.8746944665908813,
          0.8808394074440002,
          0.8825976252555847,
          0.8659182786941528,
          0.8765340447425842,
          0.8794926404953003,
          0.8684983253479004,
          0.8669114708900452,
          0.8744003772735596,
          0.8676278591156006,
          0.8658921718597412,
          0.8609579205513,
          0.858111560344696,
          0.8532997369766235,
          0.8585422039031982,
          0.8706136345863342,
          0.8660969138145447,
          0.8554484844207764,
          0.8559263944625854,
          0.8628695011138916,
          0.8548322916030884,
          0.8579098582267761,
          0.848555862903595,
          0.859632670879364,
          0.8552296161651611,
          0.8530471920967102,
          0.8521256446838379,
          0.851332426071167,
          0.8514791131019592,
          0.852735698223114,
          0.8569464087486267,
          0.8576453924179077,
          0.8410096764564514,
          0.8498680591583252,
          0.8413091897964478,
          0.8382691740989685,
          0.8524834513664246,
          0.837001621723175,
          0.8482016324996948,
          0.8433309197425842,
          0.8391364812850952,
          0.835689902305603,
          0.8377231955528259,
          0.8380810022354126,
          0.8385351300239563,
          0.8356744647026062,
          0.8289474248886108,
          0.8447390794754028,
          0.8334737420082092,
          0.8280519843101501,
          0.8336942195892334,
          0.823972761631012,
          0.8253684043884277,
          0.8279549479484558,
          0.8289105892181396,
          0.8364922404289246,
          0.8280848860740662,
          0.8308780193328857,
          0.8358284831047058,
          0.8272968530654907,
          0.821294903755188,
          0.8232364654541016,
          0.8333761096000671,
          0.8208419680595398,
          0.8219716548919678,
          0.8287713527679443,
          0.8140650987625122,
          0.8107817769050598,
          0.8240587711334229,
          0.8186091780662537,
          0.8208083510398865,
          0.8107870221138,
          0.8134760856628418,
          0.8108258247375488,
          0.8095253109931946,
          0.814677894115448,
          0.8216233253479004,
          0.8151237368583679,
          0.8164969086647034,
          0.8157011866569519,
          0.8113844990730286,
          0.8126233220100403,
          0.8155865669250488,
          0.8096867799758911,
          0.805128812789917,
          0.800079882144928,
          0.8057544231414795,
          0.8100075125694275,
          0.7980920076370239,
          0.7976420521736145,
          0.813185453414917,
          0.8085121512413025,
          0.80769282579422,
          0.7894161939620972,
          0.8045023679733276,
          0.7992827892303467,
          0.8177812099456787,
          0.8026763200759888,
          0.796016275882721,
          0.7992064952850342,
          0.8106006979942322,
          0.7910292148590088,
          0.7951259613037109,
          0.7937289476394653,
          0.7913120985031128,
          0.7968822717666626,
          0.7880797982215881,
          0.7908953428268433,
          0.7999780774116516,
          0.8029283881187439,
          0.7915207147598267,
          0.7933367490768433,
          0.8016435503959656,
          0.7920288443565369,
          0.8033615946769714,
          0.7962604761123657,
          0.7936993837356567,
          0.7865191698074341,
          0.7898746728897095,
          0.788061261177063,
          0.790463387966156,
          0.7859357595443726,
          0.79449063539505,
          0.7903823852539062,
          0.7912770509719849,
          0.7831845879554749,
          0.7875387072563171,
          0.7988954186439514,
          0.7907887697219849,
          0.7828192114830017,
          0.8024393916130066,
          0.7860449552536011,
          0.7873969078063965,
          0.7886937856674194,
          0.777931272983551,
          0.7832028269767761,
          0.7900896072387695,
          0.7896847724914551,
          0.7800300717353821,
          0.7799088358879089,
          0.7888696789741516,
          0.7787323594093323,
          0.7842622995376587,
          0.7740163207054138,
          0.783427894115448,
          0.7806763648986816,
          0.7781966924667358,
          0.7822802662849426,
          0.7815666198730469,
          0.7759215235710144,
          0.7872020602226257,
          0.7805176377296448,
          0.7791401147842407,
          0.7811717391014099,
          0.7865269184112549,
          0.7875721454620361,
          0.7757472395896912,
          0.7799177169799805,
          0.7846702933311462,
          0.7767969369888306,
          0.7805249691009521,
          0.7703201174736023,
          0.7788316607475281,
          0.7831923365592957,
          0.7728551030158997,
          0.7767332792282104,
          0.7868686318397522,
          0.7802830934524536,
          0.7738739848136902,
          0.7698850631713867,
          0.7653253078460693,
          0.7675922513008118,
          0.7658663988113403,
          0.7765922546386719,
          0.7753766179084778,
          0.7792451977729797,
          0.7573032975196838,
          0.7655391693115234,
          0.7743815183639526,
          0.7644018530845642,
          0.7770090699195862,
          0.764677882194519,
          0.7648286819458008,
          0.7625159621238708,
          0.7660627365112305,
          0.7630835771560669,
          0.7642970681190491,
          0.7622058987617493,
          0.7708597779273987,
          0.758604884147644,
          0.7613622546195984,
          0.7699296474456787,
          0.7668653726577759,
          0.7587171196937561,
          0.7654450535774231,
          0.7745357751846313,
          0.7731731534004211,
          0.7616474032402039,
          0.7631203532218933,
          0.7648063898086548,
          0.7621477842330933,
          0.7559986114501953,
          0.7602835297584534,
          0.772697389125824,
          0.7716109156608582,
          0.7687532305717468,
          0.7730528712272644,
          0.7738187909126282,
          0.7572882771492004,
          0.7592434883117676,
          0.7599988579750061,
          0.7667391896247864,
          0.7470400929450989,
          0.7566342353820801,
          0.751697838306427,
          0.7521998286247253,
          0.7747998833656311,
          0.7648559808731079,
          0.7573873996734619,
          0.7609944343566895,
          0.7570712566375732,
          0.7563413977622986,
          0.7619147896766663,
          0.7720389366149902,
          0.7592830061912537,
          0.7543352246284485,
          0.7442702651023865,
          0.7506964802742004,
          0.7527494430541992,
          0.7631180882453918,
          0.7397920489311218,
          0.759233832359314,
          0.7618052959442139,
          0.7618188858032227,
          0.7538130879402161,
          0.7450363039970398,
          0.7551629543304443,
          0.7542328238487244,
          0.7524968385696411,
          0.7584272027015686,
          0.7490156888961792,
          0.7543329000473022,
          0.7473036646842957,
          0.7557674050331116,
          0.7568922638893127,
          0.7509303092956543,
          0.7570661306381226,
          0.7534905076026917,
          0.7527850270271301,
          0.7576394081115723,
          0.7457960844039917,
          0.7510701417922974,
          0.7601828575134277,
          0.7469328045845032,
          0.7540653944015503,
          0.7496039271354675,
          0.7541176080703735,
          0.7562986016273499,
          0.7554733157157898,
          0.75336092710495,
          0.7482986450195312,
          0.7569213509559631,
          0.7522185444831848,
          0.7497487664222717,
          0.743560791015625,
          0.7410604357719421,
          0.740544319152832,
          0.7487361431121826,
          0.7421022653579712,
          0.7603603601455688,
          0.7506751418113708,
          0.764198362827301,
          0.7485868334770203,
          0.7479526996612549,
          0.7506593465805054,
          0.7604973912239075,
          0.7564982771873474,
          0.7446361780166626,
          0.7455664873123169,
          0.7414227724075317,
          0.7474204897880554,
          0.743471622467041,
          0.74400395154953,
          0.7408886551856995,
          0.7455917596817017,
          0.74464350938797,
          0.7435139417648315,
          0.7442432045936584,
          0.7387166619300842,
          0.7387253046035767,
          0.7353318333625793,
          0.7415504455566406,
          0.7527226805686951,
          0.7447196841239929,
          0.7518351674079895,
          0.7574186325073242,
          0.7367672324180603,
          0.7346177697181702,
          0.7547479271888733,
          0.7484192848205566,
          0.7371517419815063,
          0.7508565783500671,
          0.7395293116569519,
          0.7485900521278381,
          0.7380269765853882,
          0.7492559552192688,
          0.7510843276977539,
          0.7354157567024231,
          0.7384523153305054,
          0.738922655582428,
          0.7382258176803589,
          0.7405555248260498,
          0.7396320700645447,
          0.7394668459892273,
          0.7464359402656555,
          0.7265084385871887,
          0.737126350402832,
          0.7473529577255249,
          0.7509864568710327,
          0.7350465059280396,
          0.7294600605964661,
          0.7526707053184509,
          0.7443827986717224,
          0.7380933165550232,
          0.7373059988021851,
          0.7434831261634827,
          0.734880805015564,
          0.7455743551254272,
          0.734921932220459,
          0.7490344047546387,
          0.7415329813957214,
          0.7353161573410034,
          0.7455256581306458,
          0.7342278361320496,
          0.7393411993980408,
          0.7340827584266663,
          0.7459681034088135,
          0.7236823439598083,
          0.7417684197425842,
          0.7369034290313721,
          0.7335910797119141,
          0.7369310259819031,
          0.7418891191482544,
          0.7241979837417603,
          0.7397550344467163,
          0.7400047183036804,
          0.7375837564468384,
          0.7328574657440186,
          0.7367649078369141,
          0.734913170337677,
          0.7366992235183716,
          0.7246683835983276,
          0.7362045049667358,
          0.7388337850570679,
          0.7283385396003723,
          0.7371916770935059,
          0.7313241362571716,
          0.7317315340042114,
          0.7310447692871094,
          0.7329299449920654,
          0.7375088930130005,
          0.7249862551689148,
          0.7439609169960022,
          0.7280092835426331,
          0.7281806468963623,
          0.7303730249404907,
          0.7340222597122192,
          0.7398896217346191,
          0.7243222594261169,
          0.734452486038208,
          0.7311125993728638
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=val_loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "val_loss",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "val_loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          1.2125366926193237,
          1.2928797006607056,
          1.1276508569717407,
          1.1239632368087769,
          1.0786020755767822,
          1.1399099826812744,
          1.0715967416763306,
          1.063278317451477,
          1.100484848022461,
          1.047947883605957,
          1.0428900718688965,
          1.0895549058914185,
          1.0712817907333374,
          1.0374809503555298,
          1.0751330852508545,
          1.0757778882980347,
          1.0570366382598877,
          1.0624217987060547,
          1.055423378944397,
          1.0251671075820923,
          1.064919352531433,
          1.0531471967697144,
          1.0486700534820557,
          1.0519784688949585,
          1.0371760129928589,
          1.0525610446929932,
          1.0398119688034058,
          1.0329158306121826,
          1.0688183307647705,
          1.0501348972320557,
          1.0439220666885376,
          1.0241284370422363,
          1.0319504737854004,
          1.060039758682251,
          1.0463573932647705,
          1.028962254524231,
          1.0264759063720703,
          1.0156755447387695,
          1.009634017944336,
          1.0108064413070679,
          1.0348480939865112,
          0.9942842721939087,
          1.0446048974990845,
          1.0377674102783203,
          0.9879883527755737,
          1.0497493743896484,
          1.02572762966156,
          1.041822910308838,
          0.9871971607208252,
          1.001721978187561,
          1.0237295627593994,
          1.026968240737915,
          1.0326237678527832,
          1.0483354330062866,
          1.018423080444336,
          1.0297534465789795,
          1.0442478656768799,
          1.0041766166687012,
          1.0174120664596558,
          1.041964054107666,
          1.011173129081726,
          1.0831899642944336,
          1.0147544145584106,
          0.9780887365341187,
          0.9790323972702026,
          1.056066870689392,
          1.0456697940826416,
          0.9588375091552734,
          0.9911307096481323,
          1.0049406290054321,
          1.0023679733276367,
          0.9748886227607727,
          0.9594955444335938,
          0.9610755443572998,
          0.9648734331130981,
          1.009337067604065,
          0.9874780178070068,
          0.9921582341194153,
          1.0000288486480713,
          0.9682059288024902,
          0.9653063416481018,
          0.9527606964111328,
          0.989204466342926,
          0.9721519351005554,
          0.976226270198822,
          0.9355512857437134,
          0.9809293746948242,
          0.9846398234367371,
          0.975986123085022,
          0.9509953260421753,
          0.9843182563781738,
          1.0179424285888672,
          0.9669111967086792,
          0.9391402006149292,
          0.9533392786979675,
          0.9783326983451843,
          0.9395861029624939,
          0.9324374198913574,
          0.9829520583152771,
          0.9545077085494995,
          1.0145208835601807,
          0.9359414577484131,
          0.9215490221977234,
          0.9246682524681091,
          0.9392799139022827,
          0.8972625136375427,
          0.964565634727478,
          1.0055410861968994,
          0.9762324094772339,
          0.9353954195976257,
          0.943166196346283,
          0.9023967385292053,
          0.9585734009742737,
          0.9241164922714233,
          0.9362705945968628,
          0.889814019203186,
          0.9137499332427979,
          0.9559333324432373,
          0.9494548439979553,
          0.9185436964035034,
          0.9025059342384338,
          0.92484450340271,
          0.9198901653289795,
          0.9036462903022766,
          0.9166314005851746,
          0.9366651773452759,
          0.9285237789154053,
          0.9187126159667969,
          0.9520890712738037,
          0.9137783646583557,
          0.9047105312347412,
          0.8898636698722839,
          0.9274799227714539,
          0.8944893479347229,
          0.9046332836151123,
          0.9332255125045776,
          0.9231467843055725,
          0.9128939509391785,
          0.9181968569755554,
          0.9047819375991821,
          0.8908926248550415,
          0.8943977355957031,
          0.8949834108352661,
          0.881636917591095,
          0.8767831921577454,
          0.8991718292236328,
          0.8982646465301514,
          0.8740274310112,
          0.8963357210159302,
          0.8835080862045288,
          0.9119991660118103,
          0.911090075969696,
          0.8855577111244202,
          0.8845605850219727,
          0.8654311895370483,
          0.8596192598342896,
          0.9004409909248352,
          0.8843691349029541,
          0.8501867055892944,
          0.8578215837478638,
          0.862606406211853,
          0.8536435961723328,
          0.88455730676651,
          0.8992177844047546,
          0.8705389499664307,
          0.8819624185562134,
          0.8846307396888733,
          0.8814032077789307,
          0.8851407766342163,
          0.8829193115234375,
          0.872292160987854,
          0.8616777658462524,
          0.8896501660346985,
          0.8788074254989624,
          0.8461897969245911,
          0.8646039366722107,
          0.8788048624992371,
          0.8369320631027222,
          0.8779299855232239,
          0.8562519550323486,
          0.8453043103218079,
          0.9003374576568604,
          0.8907862901687622,
          0.9105345010757446,
          0.8587585091590881,
          0.880518913269043,
          0.8509377837181091,
          0.9135205745697021,
          0.9102511405944824,
          0.8797873258590698,
          0.8371008634567261,
          0.8379265069961548,
          0.8759250044822693,
          0.8533228635787964,
          0.8971008658409119,
          0.8592622876167297,
          0.8808953762054443,
          0.8592047691345215,
          0.8648001551628113,
          0.8820328116416931,
          0.8355052471160889,
          0.862876832485199,
          0.8986305594444275,
          0.8310823440551758,
          0.8506020903587341,
          0.8366333842277527,
          0.8733893036842346,
          0.8179898262023926,
          0.9064810276031494,
          0.8811783194541931,
          0.9204327464103699,
          0.908524215221405,
          0.8552010655403137,
          0.863256573677063,
          0.8240160942077637,
          0.8833257555961609,
          0.8634767532348633,
          0.8387475609779358,
          0.8581409454345703,
          0.8560823202133179,
          0.8040874004364014,
          0.8886094689369202,
          0.8562917709350586,
          0.8475627899169922,
          0.8546563982963562,
          0.8483688831329346,
          0.8307202458381653,
          0.8533252477645874,
          0.8468115925788879,
          0.8534349799156189,
          0.8500324487686157,
          0.8497922420501709,
          0.8434097766876221,
          0.8359266519546509,
          0.8476651906967163,
          0.8406152725219727,
          0.8377068042755127,
          0.8393571972846985,
          0.8626238703727722,
          0.8952755928039551,
          0.8716799020767212,
          0.8092797994613647,
          0.8404593467712402,
          0.8318149447441101,
          0.8492388725280762,
          0.9230096340179443,
          0.8584164977073669,
          0.7948761582374573,
          0.8132973313331604,
          0.830976665019989,
          0.8068100214004517,
          0.789394736289978,
          0.8199131488800049,
          0.8318694829940796,
          0.8169993758201599,
          0.8857499957084656,
          0.8538364171981812,
          0.8910985589027405,
          0.8780760169029236,
          0.8200127482414246,
          0.8555588126182556,
          0.8334341645240784,
          0.8167874813079834,
          0.7982987761497498,
          0.8785733580589294,
          0.8138839602470398,
          0.8383530378341675,
          0.8105243444442749,
          0.8396671414375305,
          0.8300186991691589,
          0.8276503682136536,
          0.8789876103401184,
          0.7952679395675659,
          0.8248249292373657,
          0.8362822532653809,
          0.8376870155334473,
          0.8691371083259583,
          0.8102188110351562,
          0.8475466370582581,
          0.8098039031028748,
          0.8632471561431885,
          0.8271864652633667,
          0.8535653948783875,
          0.824205219745636,
          0.8727746605873108,
          0.803863525390625,
          0.8190923929214478,
          0.8217374682426453,
          0.8269208669662476,
          0.8101111650466919,
          0.8240548968315125,
          0.8331061005592346,
          0.8495233058929443,
          0.8085005879402161,
          0.8479666709899902,
          0.7934853434562683,
          0.8088343739509583,
          0.8125835657119751,
          0.8651723861694336,
          0.8204166889190674,
          0.8763050436973572,
          0.8473567366600037,
          0.832214891910553,
          0.8531375527381897,
          0.8426034450531006,
          0.8536786437034607,
          0.8081989288330078,
          0.8624187111854553,
          0.8281261920928955,
          0.8271922469139099,
          0.8533668518066406,
          0.8418176174163818,
          0.8264743685722351,
          0.8536120057106018,
          0.8006604313850403,
          0.8361378908157349,
          0.793526291847229,
          0.8391695618629456,
          0.8296017646789551,
          0.8556630611419678,
          0.8202598094940186,
          0.8388774394989014,
          0.8631236553192139,
          0.855277955532074,
          0.8364239931106567,
          0.8279322385787964,
          0.8038896322250366,
          0.8140000700950623,
          0.8341843485832214,
          0.8185175657272339,
          0.8253679871559143,
          0.8347882032394409,
          0.8288354277610779,
          0.8255134224891663,
          0.8144155144691467,
          0.8622576594352722,
          0.802861750125885,
          0.8149546384811401,
          0.8300971984863281,
          0.8603799343109131,
          0.8624631762504578,
          0.8374322056770325,
          0.8440951704978943,
          0.8314096927642822,
          0.8270245790481567,
          0.7888498306274414,
          0.8341853618621826,
          0.8377836346626282,
          0.8412697911262512,
          0.8410443663597107,
          0.8270455002784729,
          0.8329899311065674,
          0.834987461566925,
          0.8209793567657471,
          0.8012381196022034,
          0.8451642990112305,
          0.809278666973114,
          0.8125050067901611,
          0.8041141629219055,
          0.7930595278739929,
          0.8143040537834167,
          0.8394391536712646,
          0.8305134177207947,
          0.8231725692749023,
          0.7957778573036194,
          0.8267958164215088,
          0.8839810490608215,
          0.8249291181564331,
          0.8224324584007263,
          0.8282845616340637,
          0.851857602596283,
          0.8036015629768372,
          0.8015151619911194,
          0.8142696022987366,
          0.8636524677276611,
          0.7818729877471924,
          0.8232927322387695,
          0.790432870388031,
          0.8302069306373596,
          0.8778381943702698,
          0.862056314945221,
          0.8080073595046997,
          0.8332380652427673,
          0.7893509268760681,
          0.8264209032058716,
          0.8794041872024536,
          0.7910354733467102,
          0.8430265188217163,
          0.8029707670211792,
          0.811119556427002,
          0.8118947744369507,
          0.8481258749961853,
          0.8045462369918823,
          0.8324785828590393,
          0.8089934587478638,
          0.8377271294593811,
          0.7893849015235901,
          0.7978038191795349,
          0.7953922152519226,
          0.8279632329940796,
          0.8051578402519226,
          0.8258554935455322,
          0.8310437202453613,
          0.8146820664405823,
          0.8361494541168213,
          0.8209593892097473,
          0.8051724433898926,
          0.8276318907737732,
          0.8234947323799133,
          0.8360642790794373,
          0.8043382167816162,
          0.8224160075187683,
          0.807531476020813,
          0.8035746216773987,
          0.782081127166748,
          0.8464784622192383,
          0.8105912804603577,
          0.8049517273902893,
          0.8526841998100281,
          0.8723291754722595,
          0.8214714527130127,
          0.8593046069145203,
          0.8085463047027588,
          0.8349704742431641,
          0.7851877212524414,
          0.7986680269241333,
          0.8095841407775879,
          0.8156152963638306,
          0.8076772689819336,
          0.798021674156189,
          0.8461107611656189,
          0.8060048818588257,
          0.8306413292884827,
          0.8144896030426025,
          0.8707476854324341,
          0.8110595941543579,
          0.81705641746521,
          0.8150268793106079,
          0.810649573802948,
          0.8266462683677673,
          0.8612167835235596,
          0.8384540677070618,
          0.8092517256736755,
          0.8526343703269958,
          0.8114522695541382,
          0.7862997651100159,
          0.820881724357605,
          0.8460248112678528,
          0.7974448800086975,
          0.7995012998580933,
          0.8103539943695068,
          0.9030710458755493,
          0.8235067129135132,
          0.8243865966796875,
          0.832257866859436,
          0.8147529363632202,
          0.8615884184837341,
          0.8537140488624573,
          0.8110160827636719,
          0.8450567722320557,
          0.8051602840423584,
          0.8655089139938354,
          0.8143140077590942,
          0.8362431526184082,
          0.8145657181739807,
          0.8262995481491089,
          0.819610595703125,
          0.811276912689209,
          0.8218398094177246,
          0.8216277956962585,
          0.8040525317192078,
          0.7831014394760132,
          0.7930868268013,
          0.8156003355979919,
          0.7998088598251343,
          0.7947423458099365,
          0.8363592624664307,
          0.8196448683738708,
          0.7915003299713135,
          0.8693317174911499,
          0.8255031108856201,
          0.8338162899017334,
          0.8227375745773315,
          0.8454498648643494,
          0.8087236881256104,
          0.8272760510444641,
          0.8089173436164856,
          0.8186789751052856,
          0.8079596161842346,
          0.8172049522399902,
          0.8047316074371338,
          0.8570619821548462,
          0.7960605025291443,
          0.8225498795509338,
          0.8380147218704224,
          0.8295766115188599,
          0.814143717288971,
          0.8284950256347656,
          0.8201785087585449,
          0.8313459753990173
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = np.array(history.history['loss']).flatten()\n",
    "valf1 = np.array(history.history['val_loss']).flatten()\n",
    "px.line(pd.DataFrame(np.array([f1, valf1]).T, columns=['loss', 'val_loss'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model/MTJaw0411_500_W200_T8879/\", save_format=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unknown = None\n",
    "for u in unknownActions:\n",
    "    if not isinstance(X_unknown, np.ndarray):\n",
    "        X_unknown = u\n",
    "    else:\n",
    "        X_unknown = np.concatenate([X_unknown, u], axis=0)\n",
    "All_X = np.concatenate([X_test, X_unknown])\n",
    "All_y = np.concatenate([y_test, np.array([[0] * CLASS_NUMBER for _ in range(X_unknown.shape[0])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(All_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: undefined action, True: 5929, False: 317, Accuracy: 0.9492\n",
      "Action: up, True: 49, False: 74, Accuracy: 0.3984\n",
      "Action: down, True: 14, False: 78, Accuracy: 0.1522\n",
      "Action: left, True: 164, False: 354, Accuracy: 0.3166\n",
      "Massive prediction error times: 5, portion: 0.0141.\n",
      "Action: right, True: 177, False: 259, Accuracy: 0.4060\n",
      "Massive prediction error times: 0, portion: 0.0000.\n",
      "Action: quick touch, True: 0, False: 48, Accuracy: 0.0000\n",
      "Total True: 6333, False: 1130, Accuracy: 0.8486\n",
      "Action:left ,bad prediction times: 121\n",
      "Action:right ,bad prediction times: 134\n",
      "Action:up ,bad prediction times: 57\n",
      "Action:down ,bad prediction times: 5\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(All_y, res, belief = 0.9809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 : [0.89991, 6.226228472501272] 0.9999 0.89991\n",
      "iteration 1 : [0.979902, 6.417281152401127] 0.989901 0.979902\n",
      "iteration 2 : [0.9809019000000001, 6.4180482756576875] 0.979902 0.9809019000000001\n"
     ]
    }
   ],
   "source": [
    "#Grid search\n",
    "parameterSelection = []\n",
    "lb, ub = 0, 0.9999\n",
    "best = None\n",
    "for n in range(3):\n",
    "    for i in range(10):\n",
    "        threshold = lb + (i + 1) * 0.1 * (ub - lb)\n",
    "        parameterSelection.append([threshold, evaluate(All_y, res, uw = 6, belief = threshold, v = False)])\n",
    "    parameterSelection.sort(key=lambda x:x[1])\n",
    "    lb, ub = parameterSelection[-2][0], parameterSelection[-1][0]\n",
    "    best = parameterSelection[-1]\n",
    "    print(\"iteration\", n, \":\", best, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ConTradiction_model((CHANNEL_NUMBER, WINDOW_SIZE, 1))\n",
    "test.load_weights('./model/' + \"MTJaw0411_500_W200_T8789\" + '/')\n",
    "res = test(All_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_21_184531_l5m6r7_record_X.npy')[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(np.load('./data/2023_Mar_21_184531_l5m6r7_record_y.npy')[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, u = slicing(np.load('./data/2023_Apr_11_145938_l5m6r7_record_X.npy'), np.load('./data/2023_Apr_11_145938_l5m6r7_record_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model(np.concatenate([X, u]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: undefined action, True: 243, False: 4, Accuracy: 0.9838\n",
      "Action: up, True: 0, False: 0, Accuracy: 0.0000\n",
      "Action: down, True: 0, False: 0, Accuracy: 0.0000\n",
      "Action: left, True: 36, False: 75, Accuracy: 0.3243\n",
      "Massive prediction error times: 0, portion: 0.0000.\n",
      "Action: right, True: 28, False: 56, Accuracy: 0.3333\n",
      "Massive prediction error times: 0, portion: 0.0000.\n",
      "Action: quick touch, True: 0, False: 0, Accuracy: 0.0000\n",
      "Total True: 307, False: 135, Accuracy: 0.6946\n",
      "Action:left ,bad prediction times: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.609055822676756"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(np.concatenate([y, np.zeros((u.shape[0], CLASS_NUMBER))]), r, belief = 0.9809)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
